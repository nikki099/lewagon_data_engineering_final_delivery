{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66bef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import http.client\n",
    "import urllib.parse\n",
    "import snowflake.connector\n",
    "from sqlalchemy import create_engine\n",
    "from joblib import Parallel, delayed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b331d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "snowflake_password = os.getenv('SNOWFLAKE_PASSWORD')\n",
    "SERVICE_ACCOUNT_FILE = os.getenv('SERVICE_ACCOUNT_FILE')\n",
    "\n",
    "# Rate limit control\n",
    "rate_lock = threading.Lock()\n",
    "last_request_time = 0\n",
    "\n",
    "\n",
    "#Gsheet credential\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "]\n",
    "creds = Credentials.from_service_account_file(\n",
    "    SERVICE_ACCOUNT_FILE,\n",
    "    scopes=SCOPES)\n",
    "\n",
    "gc = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23484c13",
   "metadata": {},
   "source": [
    "## Connect to  Snowflake database for raw data initial processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish a connection to Snowflake\n",
    "\n",
    "def connect_to_snowflake():\n",
    "    try:\n",
    "\n",
    "        conn = snowflake.connector.connect(\n",
    "            user=\"NIKKILW2025\",\n",
    "            password=snowflake_password,\n",
    "            account=\"gbszkwp-by30611\",\n",
    "            warehouse=\"SNOWFLAKE_LEARNING_WH\",\n",
    "            database=\"linkedin_db\",\n",
    "            schema=\"linkedin_raw\"\n",
    "        )\n",
    "        print(\"Connection to Snowflake established successfully.\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Snowflake: {e}\")\n",
    "        return None\n",
    "\n",
    "conn = connect_to_snowflake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd403f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gspread.worksheet.Worksheet"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spreadsheet = gc.open_by_key(\"1I-Q-nFlp--jLXYSrS8TuOvxZO62KvhTdDSn1iLDBTkQ\")\n",
    "worksheets = spreadsheet.worksheets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805bd9c2",
   "metadata": {},
   "source": [
    "### 1. Read in DA Job Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba327462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Worksheet 'da_description' id:819730630>\n",
      "<Worksheet 'de_description' id:1530019367>\n",
      "<Worksheet 'ds_description' id:244877389>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>About the job\\r\\nJoin an exciting technology s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>About the job\\r\\nYum! Brands is the world’s la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>About the job\\r\\nJoin Vertical Scope Group’s (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Job Description\\r\\n\\r\\nThe Operations Research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>About the job\\r\\nJoin Australia's leading onli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Job_Title                                    Job_Description\n",
       "0  Data Scientist  About the job\\r\\nJoin an exciting technology s...\n",
       "1  Data Scientist  About the job\\r\\nYum! Brands is the world’s la...\n",
       "2  Data Scientist  About the job\\r\\nJoin Vertical Scope Group’s (...\n",
       "3  Data Scientist  Job Description\\r\\n\\r\\nThe Operations Research...\n",
       "4  Data Scientist  About the job\\r\\nJoin Australia's leading onli..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_job_descriptions():\n",
    "    spreadsheet = gc.open_by_key(\"1I-Q-nFlp--jLXYSrS8TuOvxZO62KvhTdDSn1iLDBTkQ\")\n",
    "    worksheets = spreadsheet.worksheets()\n",
    "\n",
    "    df_jobs = pd.DataFrame()\n",
    "    for worksheet in worksheets[1:]: #omit the first sheet/master sheet\n",
    "        print(worksheet)\n",
    "        all_data = worksheet.get_all_values()\n",
    "        headers = all_data[0]\n",
    "        rows = all_data[1:]\n",
    "        df_data = pd.DataFrame(rows, columns=headers)\n",
    "        df_data = df_data.drop_duplicates(subset=['Job_Description'], keep='first')\n",
    "        df_jobs = pd.concat([df_data, df_jobs],ignore_index=True)\n",
    "    return df_jobs\n",
    "\n",
    "df_jobs = get_job_descriptions()\n",
    "df_jobs.shape\n",
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7eaed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data Analyst': [\"The Suburban Connect consortium, comprising CPB Contractors, ACCIONA and Ghella, have been selected by the Victorian Government to deliver the first major tunnelling package on the Suburban Rail Loop (SRL) East project.The package includes construction of a 16-kilometre section of the project’s twin tunnels, including tunnelling between Cheltenham and Glen Waverley, 55 safety cross passages between the tunnels, station boxes at Clayton and Monash and the construction of portals – the entrances and exits to the tunnels – at the stabling facility.About The OpportunityJoin our team as a Health & Safety Data Analyst and you will be pivotal in helping us to understand our health and safety risk profile and finding new and innovative ways to achieve an industry leading health and safety performance.The Key Responsibilities Of The Role Include      Sourcing health and safety data from various inputs and maintaining the health and safety data lake.Creating dynamic dashboards to communicate health and safety performance data in way that is aligned to end user and stakeholder needs.Supporting all health and safety reporting.Analysing health and safety data and trends to provide predictive advice on areas for improvement, likely sources of future incidents and assurance focus areas.Creating intuitive forms and content for people to interact with with focus on data being accurate and actionable.Support the wider health and safety team and project by digitising and simplifying health and safety processes, forms, and tools.Working with stakeholders to determine their needs and how health and safety data, analytics and reporting can be creating to meet their needs.Lead the delivery of several technical innovations in the management of health and safety through the provision of advanced data management and analytics.Automating data collection, transformation, and reporting processes. e.g. Experience using PowerQuery, PowerAutomate, PowerBI Dataflows, PowerShell, and MS Forms. We Are Looking For Someone With      Experience in working with health and safety data, key metrics, and performance criteria.Experience as a PowerBI developer (i.e. creating dashboards & programming custom visuals using PowerBI's DAX language).Superior knowledge of Excel and when Excel is a better choice compared to PowerBI and vice versa.Experience using PowerQuery, PowerAutomate, PowerBI Dataflows, PowerShell, and MS Forms.Experience integrating data from various internal sources.Solid experience with cleansing data, anticipating bugs caused by user input errors and has experience in forecasting ways to manage and overcome incomplete data sets. What We Offer      Shape Melbourne's Future: Be part of building a landmark infrastructure project that will leave a lasting mark on the city’s public transport networkInvest in Your Career: Long-term stability with a clear growth trajectory, fueled by our pipeline of future projectsThrive in Collaboration: Immerse yourself in a diverse, inclusive culture that values innovative problem-solving and teamworkContinuously Learn & Grow: Access professional development opportunities and share your unique expertise within a supportive environment To submit your application, please click on the link below.   \\nAbout Us:At Data Insight Labs Pvt Ltd, we help businesses turn raw data into strategic insights. We work across industries including government, finance, healthcare, and transport—delivering analytics solutions that drive impact. Our team is passionate about unlocking value from data and building scalable, modern data environments.About the Role:We are looking for a detail-oriented and business-savvy Data Analyst to join our growing analytics team. You’ll work directly with clients and stakeholders to explore data, build insightful dashboards, and support decision-making with clear, actionable analytics.Key Responsibilities:      Extract, clean, and analyze data from cloud-based sources (Azure SQL, Snowflake, AWS)Design and develop dynamic dashboards and reports using Power BI, Tableau, or QlikWrite efficient SQL queries to gather and manipulate datasetsCollaborate with stakeholders to translate business needs into data requirements and reporting solutionsIdentify data quality issues and support data governance practicesPresent findings clearly to both technical and non-technical audiences. Skills & Experience:      Bachelor’s degree in Data Science, Computer Science, Statistics, or a related field2+ years of experience in a data analyst or business intelligence roleStrong SQL skills (T-SQL, PostgreSQL, or similar)Proven experience with tools such as Power BI, Azure Data Factory, Snowflake, or TableauKnowledge of data modeling concepts and DAX (preferred)Familiarity with cloud data platforms: Azure, AWS, or GCPExcellent communication and stakeholder engagement skills. What We Offer:      Competitive salary package: AUD 90,000 – 110,000Flexible working arrangements (hybrid/remote options available)Exposure to cutting-edge cloud technologies and real-world data projectsLearning & development budget to support certifications and trainingA collaborative and supportive team cultureOpportunity to work across diverse industries with high-impact clients. At Data Insight Labs Pvt Ltd, we value diversity and are committed to creating an inclusive workplace. We welcome applicants from all backgrounds.                                         \\n\\nNA\\nSAP EAM Functional Analyst – 6-Month Contract | Remote (Sydney or Melbourne)Join a major ERP transformation program as a Functional Data Analyst. We're looking for an experienced SAP EAM professional to support a critical data migration initiative. This fully remote contract (from Sydney or Melbourne) will run for an initial 6 months, with potential to extend.What You’ll Be Doing: You'll be part of a dedicated Data Migration Team, working across multiple projects to ensure the successful migration of data from legacy systems into SAP. Your role will involve close collaboration with Product Owners, Functional Specialists, SMEs, and developers to design, build, test, and validate complex data migration processes.Key Responsibilities:      Lead data standards workshops and define governance rules for master data objectsCreate functional designs for data migration in partnership with business SMEsProvide expert support to migration developers during the build processAssist SMEs in data preparation and validationIdentify dependencies and support project planningContribute to data cleansing efforts and perform validation of data loadsResolve data migration defects through analysis and remediationCommunicate progress and issues to project leads and management What We’re Looking For:      5+ years of hands-on functional experience with SAP EAMDeep understanding of SAP EAM configuration, processes, and dataStrong knowledge of integration points with other SAP modules including FinanceConfidence working with data outside EAM, such as Class & Characteristics, Sales Orders, QM dataProven experience facilitating workshops and designing end-to-end data solutions This role is ideal for someone who thrives in dynamic project environments, enjoys solving complex data challenges, and is confident working across multiple SAP domains.To apply please send your resume or reach out to Nat on 0430 292 875 or Natalie@blackroc.co                 \\r\\n\\nThis pay rate is inclusive of mandatory 25% casual loading      Robust financial reporting3 month contract / CBDCBD / Flexible work arrangements Our client is looking for a detailed oriented data analyst to join their Finance team and play a key role in the preparation of year-end financial statements. You will support the Finance and Procurement teams by providing expert data analysis services to help prepare the Procurement Reporting for the 30 June Financial Statements.  This will involve extracting and analysing large datasets from the general ledger and procurement systems and identifying and reconciling discrepancies between financial and procurement data sources. You will classify and present contract expenditure and document procedures and maintain audit-ready records supporting financial statement disclosures.Suitable candidates will have proven experience in data analysis and modelling, ideally within large commercial or government environments with advanced proficiency in tools such as Microsoft SQL, Excel, and other data analysis software. You will have strong analytical thinking and attention to detail when working with large, complex datasets and exceptional problem-solving abilities and the tenacity to investigate and resolve data variances. A working knowledge of Financial Accounting principles and practices is desirable.Enquiries to Shiv on 0439074391                 \\n\\nThe Pepperstone story started in 2010. We know what it's like to trade the world's markets. Our team describes us as a place for the curious and the driven, and we like to do things a little differently; as a transformative global fintech we're digital, nimble, connected, and united in our vision to create a better way to trade. \\r\\nWe thrive on progress - for our clients and for ourselves. Our organisational culture is ever-evolving, vibrant, diverse, global and results focused. \\r\\nYou'll find our 550+ team currently across 11 locations and 9 time zones. \\r\\n\\r\\n\\r\\nThe Role \\r\\n\\r\\nAs a Transaction Reporting Analyst at Pepperstone, you'll spearhead the ongoing enhancement of our global transaction reporting responsibilities. In this essential position, you will offer your specialized knowledge across various jurisdictions, ensuring that our reporting is precise, prompt, and compliant. You will take charge of incorporating new reporting providers and play a vital role in refining our existing control frameworks.\\r\\nYour expertise will be crucial in fostering compliance and driving innovation within the team, ensuring seamless operations as we adapt to changing regulations. You will also work closely with our Legal and Compliance teams to keep our processes robust and current.\\r\\nThis is a fantastic opportunity to create a real impact and advance your career in a vibrant, forward-thinking environment.\\r\\n\\r\\n\\r\\nAs our Data Analyst - Transaction Reporting, your key responsibilities include, but may not be limited to: \\r\\n\\r\\n Daily Reporting Management: Monitor automated processes and report generation, verify that the transaction repository processes reports correctly, resolve rejections, and escalate issues to the relevant teams. Collaborate with our compliance team to manage potential changes in ASIC, EMIR, MiFID, or other applicable regulations\\r\\n Data Management & Reporting: Use SQL and Excel to analyze data, generate reports, and deliver actionable items. Maintain and update transaction reporting processes to ensure data accuracy and regulatory compliance\\r\\n Collaboration: Work closely with data engineers, compliance teams, and external vendors to streamline operations. Assist in integrating new reporting tools and improving existing systems\\r\\n Prioritization & Organization: Recognize and prioritize critical tasks to meet daily deadlines and regulatory requirements. Support ongoing process enhancements and help drive efficiency across the team\\r\\nAbout You\\r\\n\\r\\n Technical Skills: Proficiency in SQL and advanced Excel\\r\\n Organizational Abilities: Strong attention to detail and the ability to manage multiple priorities in a fast-paced environment\\r\\n Collaboration: Excellent communication skills and proven experience working with cross-functional teams (data engineers, compliance, external vendors)\\r\\n Experience: Previous experience in transaction reporting or similar roles is desirable, but comprehensive training will be provided for the right candidate\\r\\nWhy you will enjoy working with us\\r\\n\\r\\n Competitive salary structure including company bonus scheme \\r\\nGenuinely collaborative and friendly culture \\r\\nFlexible and hybrid working \\r\\nRemote working option - work from anywhere for up to 6 weeks per year, in addition to hybrid working as standard \\r\\nOngoing personal development & learning opportunities \\r\\n15 weeks paid primary carers parental leave & 4 weeks paid secondary carers leave \\r\\n3 paid volunteering days per year & Workplace Giving Program \\r\\nFrequent events and celebrations including a standard weekly social \\r\\nBeautifully renovated large office at Collins Square - 727 Collins Street, Melbourne \\r\\nBest in class end of trip facilities including bicycle parking, change rooms & showers \\r\\nA full stocked kitchen, onsite coffee machines with locally sourced coffee beans (this is Melbourne after all) and curated specialty teas\\nAbout the job\\r\\nWe are currently seeking an experienced Financial Data Analyst to support financial year-end reporting processes.\\r\\nKey Responsibilities:\\r\\n\\r\\nPrepare detailed expenditure reports for internal and external entities.\\r\\n\\r\\nExtract and analyse financial data from databases and general ledger systems using SQL.\\r\\n\\r\\nConduct comprehensive transaction reviews to ensure accuracy and completeness.\\r\\n\\r\\nPerform reconciliations and identify discrepancies across multiple systems.\\r\\n\\r\\nInvestigate variances to support accurate and auditable financial reporting.\\r\\n\\r\\nAssist in the preparation of financial note disclosures and audit-ready documentation.\\r\\n\\r\\nCollaborate with procurement teams to validate payment listings and ensure compliance with audit standards.\\r\\n\\r\\nExperience Required:\\r\\n\\r\\nRecent experience in a government financial accounting environment.\\r\\n\\r\\nInvolvement in financial year-end processes for a government agency.\\r\\n\\r\\nProven ability to manage and analyse large volumes of transactional data.\\r\\n\\r\\nFamiliarity with external audit requirements and public sector reporting standards.\\r\\n\\r\\nSkills & Qualifications:\\r\\n\\r\\nProficiency in SQL for data extraction and manipulation.\\r\\n\\r\\nAdvanced skills in Microsoft Excel, including pivot tables, complex formulas, macros, and data modelling.\\r\\n\\r\\nStrong analytical and problem-solving capabilities.\\r\\n\\r\\nHigh attention to detail, excellent documentation skills, and ability to work independently under deadlines.\\r\\n\\r\\nTo be considered for the role click the 'apply' button or for more information about this and other opportunities please contact Pam Kaur on + 618 7422 0615 or email pkaur@paxus.com.au and quote the above job reference number. \\r\\n\\r\\n\\r\\nPaxus values diversity and welcomes applications from Indigenous Australians, people from diverse cultural and linguistic backgrounds and people living with a disability. If you require an adjustment to the recruitment process, including the application form in an alternate format, please contact me on the above contact details.\\nAbout The RoleYou will join a highly motivated team dedicated to the safe and efficient monitoring and control of renewable energy assets. We're looking for a dedicated team player who values high standards and attention to detail, motivated to deliver first-class professional monitoring and control services.Responsibilities      Monitor & Control: Oversee assets via SCADA Systems, responding to new and developing events in the Australian National Electricity Market.Reactive & Proactive Management: React to plant and market notifications and alerts, working closely with the wider operational team.Stakeholder Communication: Interface with Transmission and Network Service Providers (TNSP), AEMO, and other entities regarding asset outages, generation constraints, or specific production/network conditions.Weather & Emergency Management: Monitor weather forecast systems, issue alerts, and handle emergency calls from asset personnel or other stakeholders.Reporting & Analysis: Contribute to the production of daily, weekly, monthly, and quarterly operational reports.Maximize Availability: Liaise with O&M Contractors to maximize asset availability and maintain operational relationships.Coordination & Improvement: Coordinate maintenance and outage performance, communicate with Contractors, TNSP, and AEMO, and continuously improve OCC systems and procedures based on Key Performance Indicators. Skills & Experience      Educational Background: Relevant degree in engineering, data science, commerce, or a similar field.Analytical Skills: Comfortable with Excel, data analysis, and communication of results. Rigorous and analytical approach.Team Player: Reliable, organized, and team-focused, thriving in a collaborative environment.Operational Experience: Experience in or comfort with working in a 24/7/365 shift-based team.Performance Under Pressure: Ability to work under intense and constant operational pressure, maintaining concentration levels for extended periods.Communication Skills: Fluent in English (written and spoken).Valued Experience: Previous experience in a trading desk or a control centre is highly regarded. How to ApplyIf you are passionate about renewable energy and believe you have the required skills and experience, we encourage you to apply now with your updated CV. For a confidential discussion, please email tom.hamilton@acrworld.com or call 02 8079 0900.                 \\r\\n\\nAbout the job   Leverage your data analysis skills to conduct detailed data analysis, insights and visualisation to inform strategic business decision-making.Be empowered to deploy your curiosity-led and problem-solving mindset to work across a range of diverse, purposeful and challenging initiatives.You’ll join a high-performing business, backing talentedindividuals. Our people are customer obsessed. They prioritise the needs and satisfaction of the customer above all else. Our mindset fosters innovation and creates strong, lasting customer relationships as we strive to be the most customer centric company in Australia and New Zealand.In the role of SeniorData Analyst - Reward, Performance & Workforce Relations role you will join the People & Culture team that’s doubling down on keeping it simple.Each Day You Will Go Above And Beyond To      Provide data analysis, insights and visualisation expertise that support the delivery of change initiatives across the RPWR portfolio initiatives.Bring your experience of managing large, complex datasets applying appropriate programming languages (SQL, Python etc.) and tools (Excel, PowerBI etc.)Extract, manipulate, and visualise data to support strategic decision-making to delivery large transformational change initiatives.Collaborate with cross-functional teams across RPWR, P&C and broader NAB stakeholders (IT, Operations, Risk etc.) building trust through clear communication to deliver data-driven solutions.Bring a hybrid delivery skillset that is adaptable to change, a curious mindset that thrives in ambiguous environments and a focus on pragmatic, risk-based approaches to getting things done. What You Will Bring      5+years’ proven experience in data analyst within Financial Services, Banking and/or Consulting in a project delivery, transformation or other execution capability.Strong analytical skills with experience handling large datasets.Demonstrated expertise in managing complex requirements across multiple cross functional initiatives with a strong focus on delivery excellence and measurable outcomes.Proven ability to design and implement innovative solutions, addressing key project/initiatives requirements and outcomes.Proficiency in PowerBI, Excel, SQL or other suitable languages for data reporting and insights generation.Strong communication skills bridging and aligning data, systems and business understanding to present data-driven insights to stakeholders.A problem-solving mindset and the ability to work collaboratively within a team.Understanding of governance practices in the financial services industry including an awareness of obligations, risks and controls. A diverse and inclusive workplace works better for everyoneWe know that our people make us who we are. That's why we have built a culture of equity and respect - where everyone feels valued and appreciated for being their authentic selves. In partnership with our multiple Employee Resource Groups (ERGs) we continue to foster an inclusive environment, where all NAB colleagues’ unique backgrounds and identities are understood, respected and celebrated. We are committed to providing an environment where you can work your way.For details on the recruitment process, and accessibility, please visit https://www.nab.com.au/about-us/careers/apply-for-job. To discuss adjustment requirements, please contact the NAB Careers team, via nab.careers@nab.com.au (please reference job number) or visit our Careers page through the link above for other contact options.Join NABIf you think this role is the right fit for you, we would love to hear from you. Please note candidate screening and interviews may be conducted prior to the closing date of the job advert. Unsolicited CVs from agencies will not be accepted.                                         \\n\\nAbout the job     Robust financial reporting3 month contract / CBDCBD / Flexible work arrangements This pay rate is inclusive of mandatory 25% casual loadingOur client is looking for a detailed oriented data analyst to join their Finance team and play a key role in the preparation of year-end financial statements. You will support the Finance and Procurement teams by providing expert data analysis services to help prepare the Procurement Reporting for the 30 June Financial Statements.This will involve extracting and analysing large datasets from the general ledger and procurement systems and identifying and reconciling discrepancies between financial and procurement data sources. You will classify and present contract expenditure and document procedures and maintain audit-ready records supporting financial statement disclosures.Suitable candidates will have proven experience in data analysis and modelling, ideally within large commercial or government environments with advanced proficiency in tools such as Microsoft SQL, Excel, and other data analysis software. You will have strong analytical thinking and attention to detail when working with large, complex datasets and exceptional problem-solving abilities and the tenacity to investigate and resolve data variances. A working knowledge of Financial Accounting principles and practices is desirable.Enquiries to Shiv on 0439074391Profession      Analytics, Business Performance                                          \\n\\nFor thousands of years, maps have provided humans with the knowledge they need to make decisions. As a Maps Evaluator, you will have the opportunity to provide ground truth for your town, city or country.At Peroptyx, we are looking for Data Analysts who will review mapping data for digital mapping applications. Your research capabilities will validate and ensure that the navigation of certain routes are accurate and safe.As part of this role you will verify that business names and opening hours are correct. You will check that the distance from a starting point to an end destination is listed accurately resulting in better user experiences.With this job you can plan your days around this highly flexible working schedule, work weekends or late evenings, all from the comfort of your own office. The flexibility of our roles minimizes the impact on your daily routine.So, whether you are a student looking to earn as you learn, a retiree looking for a new challenge a part-time/full time professional or a work from home parent, Peroptyx has the right role for you!Benefits      Work up to 20 hours per week.Earn a competitive rate of pay.Develop your research skills.Avoid the long commute.Work from the comfort of your home office.Enjoy the flexibility of setting your own working hours! Ideal Candidate      Fluent in EnglishExcellent research skills.Excellent local knowledge of your home country.Good understanding and general knowledge of the geography and culture of Australia.Analytical mindset. Job Requirements      Must be living in Australia for a minimum of 5 consecutive years.Must pass an online open-book exam that can verify your full understanding of the material and concepts.Must be willing to work a minimum of 10 hours and up to 20 hours per week depending on task availability.Good working knowledge of search engines, map applications and familiarity with social media platforms.Strong ability to learn, understand and apply multiple sets of different instructions.All work must be of an independent nature. Apply Online Today!                 \\r\\n\\nAbout the role:\\r\\n\\r\\nExperience working in Informatica Products (preferred) – Informatica EDC /Power Center/MDM\\r\\nSQL Proficiency: Strong knowledge of SQL for querying, joining, and transforming data in relational databases (e.g., Oracle, SQL Server, MySQL, PostgreSQL).\\r\\nData Governance: Understanding of data governance practices, metadata management, and data lineage for ensuring compliance and effective data management. \\r\\nData Sources and Targets: Familiarity with integrating multiple data sources (relational, flat files, XML, cloud sources) and loading data into target systems like databases, data lakes, or cloud storage.\\r\\nReference Data Models: Knowledge of creating and maintaining reference data models, data structures, and mappings for standardization across enterprise systems. \\r\\nReference Data Lifecycle Management: Understanding the lifecycle of reference data from creation and governance to archiving and version control\\nJob Description\\r\\n\\r\\n Converting the data we receive from contributing retailers and Manufacturer into actionable information\\r\\n Analysing and interpreting market data at brand and model level\\r\\n Working with large data sets and Data cleansing\\r\\n Meeting Weekly and Monthly deadlines\\r\\n Taking ownership of data quality and timeliness\\r\\n Ensure the accuracy, relevance and quality of reported data\\r\\n Effective communication skills to develop relationships with stakeholders at all levels\\r\\n Client service skills\\r\\n Strong multi-tasking skills and a team player\\r\\n\\r\\nQualifications\\r\\n\\r\\nThe ideal candidate must be highly data focused, enjoy working in a fast-paced environment and demonstrate the ability to react and adapt quickly to changing business needs with great communication skills.\\r\\n\\r\\nAlso, please take note that visa sponsorship is not provided for this role. Kindly ensure you are a local citizen or do not require any visa to work in this location.\\r\\n\\r\\nYou'll also must have:\\r\\n\\r\\n Intermediate/ Advanced Excel\\r\\n Basic/Intermediate programing skills, SQL, Python\\r\\n 1-3 years' experience in analytics, data modelling, data visualisation\\r\\n Strong logical and critical thinking with high attention to detail\\r\\n Excellent communication skills\\r\\n\\r\\nDesirable But Not Essential\\r\\n\\r\\n PowerBI\\r\\n Tableau\\r\\n\\r\\nAdditional Information\\r\\n\\r\\nOur Benefits\\r\\n\\r\\nFlexible working environment\\r\\nVolunteer time off\\r\\nLinkedIn Learning\\r\\nEmployee-Assistance-Program (EAP)\\r\\nAbout the job     Permanent, full-time positionBased in Docklands with parking included and easy access to public transportMake an impact in the payroll team About the RoleAre you ready to combine your passion for numbers and people in a role that offers both analytical depth and practical impact? Our Client is seeking a Payroll Data Analyst to join their team and play a critical role in ensuring accurate payroll delivery while unlocking powerful insights through data.This is your chance to join a fast-paced, global, forward-thinking organisation that operates across some of Australia’s most iconic hospitality, sports, and entertainment venues — and where no two days are the same.Duties And ResponsibilitiesData Analysis & Reporting      Partner with stakeholders to deliver meaningful reports and dashboards.Leverage Excel, PowerPoint, and other tools to turn data into actionable insights.Ensure data accuracy and integrity across reporting processes.Present analytical findings in a clear, compelling way to drive informed decisions. Payroll Administration      Manage end-to-end payroll processing using ADP PayForce.Interpret and apply awards, EBAs, and contracts with precision.Support HR functions including onboarding, terminations, and maintaining HRIS records.Perform audits and reconciliations to ensure payroll accuracy and compliance.Handle payroll queries and liaise with stakeholders to resolve issues efficiently. Project Support      Contribute to payroll-related projects and continuous improvement initiatives.Collaborate with external vendors and internal teams to implement enhancements.Support system upgrades and integration efforts with a solutions-focused approach. Key Tools You’ll Use      ADP PayForceHumanforce (TimeTarget)Microsoft Excel & PowerPointOneStream About You      Proven experience in payroll processing and systems (ideally ADP PayForce and HumanForce (aka TimeTarget)).Solid understanding of payroll legislation and compliance.Skilled in HR admin and maintaining accurate employee data.Advanced Excel user with strong analytical and reporting capabilities.Excellent communication and stakeholder management skills.Naturally detail-oriented with a collaborative mindset. Why Join our Client?      Work for a respected global brand with strong local presence.Grow your skills in both payroll and data analysis.Be part of a supportive and collaborative team.Enjoy staff benefits and clear development opportunities. Apply now and bring your passion for people, processes, and data to a role where you can make a real impact.                                         \\r\\n\\nWe are hiring a Data Analyst to join our Forms Express team in Geelong, Victoria!Forms Express, part of Valsoft Corporation revolutionizes the way payment solutions are delivered. Our innovative and flexible payment options are tailored to meet the unique needs of local government billing in Australia and New Zealand. Join us in 2025 as we do more with eNotices, Flexipay and AI!We are looking for a customer-centric individual with a strong analytical mindset. You must have a keen eye for data accuracy and a knack for problem-solving.If that sounds like you, then we have an exciting opportunity for you to join our growing team at Forms Express as a Data Analyst. You will play a crucial role in ensuring the smooth processing of billing data for our diverse customer base.The successful candidate will work in an on-site work model from our office in Geelong, Victoria!What Your Day Will Look LikeData Analysis and Processing: Handle data processing tasks with a focus on accuracy and attention to detail. Work closely with customer data, ensuring its cleanliness and accuracy. Map data to document templates and generate document output files for various distribution channels.Customer Interaction: Provide exceptional customer support by responding to inquiries and meeting customer deadlines. Collaborate with customers to understand their specific needs and ensure satisfaction. Deliver proofs to customers for validation before final document production.Problem Solving: Methodically resolve data issues and contribute to broader problem-solving efforts. Demonstrate persistence in addressing challenges to ensure the integrity of the billing and payment processes.Communication Skills: Possess excellent communication skills to liaise effectively with both internal teams and external customers. Collaborate with team members and work independently when necessary.Technical Skills: Utilise strong database knowledge, including familiarity with SQL Server, TSQL, and PowerBI. Develop basic web design skills, including JavaScript, HTML, and CSS. Understand basic IT infrastructure to navigate and troubleshoot related issues.Time Management: Exhibit strong time management skills to meet deadlines and handle multiple tasks efficiently.About You      Strong knowledge of SQL queries is essentialPrevious experience in a similar role is preferredA Bachelor's Degree in computer science or similar is preferredWillingness to learn and progress to more complex tasks, including basic programming codingAbility to adapt to the dynamic nature of a growing companyPositive, 'can do' attitude and plenty of energyProactive, motivated, and prepared to show initiativeStrong time management and organisational skillsMust be fluent in English, both verbal and written is essentialLegally authorised to work in Australia For more information about Forms Express, please visit our website at www.formsexpress.com.auWe thank all applicants for their interest; however, only those candidates selected for an interview will be contacted.                 \\r\\n\\nAbout the job   12 month FTCLatest TechHybrid Working HRIS Data Analyst required urgently for a 12-month FTC in SydneyYour new company  A leading Aussie organisation which operates on the cutting-edge of technology is currently undergoing a long-term HRIS transformation. Your new role  As HRIS Data Analyst, you will be migrating and maturing HRIS payroll & HR data in preparation for the implementation of new systems. You will have experience working on HR and Payroll data with one of the leading enterprise HRIS Platforms (Workday, SuccessFactors, Ramco, Oracle HCM etc.) and assisting in the design and implementation of a new data governance system. Expertise and prior experience in reporting and analysis of this HRIS data is critical. What you'll need to succeed  - Over 5 years of experience as a data analyst on HRIS, and demonstrated experience working on Data Migrations - Experience working on a leading HRIS platform (such as Workday, SuccessFactors, ADP, Oracle HCM, etc.) for a large portion of your career so far- Solid understanding of SQL, ETL, reporting (Tableau or Power BI,) within cloud environments- Knowledge of the latest data governance standards and cybersecurity requirements- Excellent teamwork, communication and problem-solving skillsWhat you need to do now  If you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV, or call us now. If this job isn't quite right for you, but you are looking for a new position, please contact us for a confidential discussion on your career.                                         \\r\\n\\nJob Title : Data AnalystLocation:SydneyMode:PermanentJob Description:      5-8 years of experience in data analysis, reporting, and business intelligence.Oversee and manage Data Governance ToolsEnsure Data sources are regularly scanned to maintain an up to date data catalogue and data lineage.Collect, manage and uplift terms and definitions with the appropriate Data Stewards.Build strong relationship across the company , working with Data owners, stewards and SMES to enforce responsibilities for maintaining data quality and standards.Review and create process maps to systems and applications.Educate and support data owners and stewards in complying with data governance policies.Should possess good Analytical and problem solving skills.Knowledge on Data Governance tools is mandatory (eg: Informatica EDC/IDMC, Securiti, Big ID, Axon etc)Should have a good understanding on Data Classification and Critical Data Elements.Power Bi and Alteryx along with SQL will be add on. Interested candidates can send their updated resume to Dhivya.Natarajan@carecone.com.au or reach me @ M: 61281982279                 \\r\\n\\nPrincess Polly is the global fashion brand at the forefront of trend-driven, lower impact and accessible apparel. Established as an online force in the Australian retail scene in 2010, Princess Polly quickly became the clothing brand found in every influencer haul, viral TikTok, Instagram OOTD and celeb street looks. Now, one of the fastest-growing online women's fashion brands in the USA, Princess Polly has been delivering the best online shopping experience to customers worldwide for over 10 years.As a part of A.K.A Brands; we are a global player with offices located in Los Angeles and Australia’s Gold Coast. With big plans ahead, we're always on the hunt for new talent to join our global team.We are growing our team based at our Burleigh Heads office and are looking for talented individuals to join the Princess Polly team. Think you can help us in our journey of becoming the best online shopping destination like...ever? If you're a motivated team player that's obsessed with all-things fashion and pop culture, we'd love to hear from you.Are you passionate about digging into data to uncover actionable insights? Do you thrive in fast-paced environments where creativity meets analytics?If you're ready to bring numbers to life and influence strategy with real impact, this could be your perfect match. We’re on the lookout for a Marketing Data Analyst to join our team. Reporting into Head of Marketing Analytics, this role is all about supporting smarter decision-making through accurate, insightful, and engaging reporting.You'll be hands-on with dashboards, SQL, and visualisation tools – helping shape the way we track performance and optimise our marketing strategy.Key responsibilities      Build, maintain and improve dashboards and reports using tools like Tableau, Looker or Power BIDevelop and optimize SQL queries to extract, transform, and analyze marketing data efficientlyEnsure data integrity and consistency across all reporting outputsAutomate recurring reports and streamline reporting processesCollaborate with cross-functional teams across the businessAssist in onboarding new marketing tech toolsProactively explore data for improvements and opportunities Skills and Experience      1–2 years in a data analytics or marketing analytics roleSolid SQL skills – confident writing complex queries and viewsExperience with data visualisation tools like Tableau, Looker, or Power BIUnderstanding of marketing performance metrics and KPIsProficiency in Excel or Google SheetsAnalytical mindset with strong attention to detailA team player who can also thrive independently If you're excited by data, thrive in fast-paced environments, and want to shape the future of marketing performance — we’d love to hear from you. We recognise that asking you to give 100% of yourself on a daily basis requires us to offer an amazing opportunity.      Competitive salary package.Flexible working arrangements.Access to a best in class parental leave and family planning program.Exceptional employee discount program.A stunning newly renovated office in beautiful Burleigh Heads;Work as part of a team of dedicated individuals focused on doing their best each day.Staff social activities.Monthly birthday celebrations. Aside from the amazing tangible benefits and perks, Princess Polly offers you the chance to make a daily impact on a global business. Princess Polly is an Equal Opportunity Employer (EOE) We're committed to a diverse and inclusive workplace and encourage applicants from all walks of life. Come join us, different makes us better.#PrincessPolly #PursueYourPassion #PrincessPollyCareers                 \\n\\n\"],\n",
       " 'Data Scientist': [\"About the job\\r\\nJoin an exciting technology start-up in AI product development. Make a positive difference working with our exciting client base which currently includes manufacturing, mining, retail, finance and property. \\r\\n\\r\\nA high level of experience in python is essential. Experience in machine learning and generative AI is essential. Full stack capability is desirable. Project execution using agile methodologies and alignment with best practice software development principles are important. Excellent communication and ability to work in a diverse team are key to being successful in this role. \\r\\n\\r\\nFlexible work arrangements are in place. \\r\\nAbout the job\\r\\nYum! Brands is the world’s largest restaurant company in terms of units, with over 55,000 restaurants across more than 150 countries and territories.\\r\\n\\r\\nWe proudly operate globally recognized brands – KFC, Pizza Hut, Taco Bell, and The Habit Burger Grill – united by a shared passion for innovation, customer experience, and delivering craveable food at scale.\\r\\nWe are seeking a talented and experienced ML to join our team.\\r\\n\\r\\n\\r\\nAs an ML Engineer you’ll be joining a team in developing and deploying high performance machine learning applications in the cloud and on the edge. You’ll be working on developing LLM based voice assistants for various use cases: telephony environment/drive through/ordering kiosk. You will contribute to design, implementation, deployment and monitoring of the solutions.\\r\\n\\r\\n\\r\\nKey responsibilities;\\r\\n\\r\\nDesign and develop applications utilizing machine learning models to solve specific business or operational problems.\\r\\nContribute to system design and architecture and scaling for cloud and edge ML applications\\r\\nDevelop ML applications: backend, cloud, deployment, observability\\r\\nOptimize performance/latency/cost of the application\\r\\nQualifications:\\r\\n\\r\\nOur ideal candidate would have 3-6 years' experience of building software applications.\\r\\nStrong programming skills in Python – Other languages like C, C++, Java, Go are well regarded but most of the coding will be done in python.\\r\\nExperience in concurrent programming\\r\\nExperience with large scale deployments\\r\\n2+ years of experience developing cloud applications (AWS/GCP/Azure)\\r\\nPreferred Qualifications:\\r\\n\\r\\nExperience building LLM based applications\\r\\nExperience building audio/voice/telephony applications\\r\\nExperience working in a Linux environment\\r\\nExperience with SQL and NoSQL databases\\r\\nKnowledge and experience in security, encryption and authentication methods\\r\\nAbout the job\\r\\nJoin Vertical Scope Group’s (VSG) Professional Services Division in Canberra as we are seeking a driven, highly skilled and competent TS PV cleared, Lead Data Scientist to join our award-winning team in supporting our Defence and Intelligence Community customers.\\r\\n\\r\\n\\r\\nAbout you:\\r\\n\\r\\nExperience with advanced analytics techniques\\r\\nExperience with data driven methodologies to extract valuable information\\r\\n\\r\\n\\r\\nAbout the role:\\r\\n\\r\\nUndertake analysis to define, structure and solve complex data problems including the generation and collection of data and meta-data from a variety of sources.\\r\\nUnderstand and utilise analytical tools including Artificial Intelligence, machine learning and predictive analytics to prioritise and solve complex problems and test models and simulations to achieve business outcomes.\\r\\nIdentify and maintain test data, scenarios, hypotheses and testing plans.\\r\\nProvide advice on data generation and analysis techniques, methods applications and outcomes.\\r\\nResponsible for the ethical use of data and interpreting and complying with legislation, the Australian Signals Directorate policy and regulatory frameworks including data management requirements.\\r\\nContribute to the management of the risks associated with the use of data including balancing security and privacy against the benefits of collecting and sharing data.\\r\\nBuild and sustain effective relationships and negotiate with customers and key stakeholders to deliver tailored recommendations.\\r\\nResolve problems using knowledge, professional judgement and initiative to identify and recommend alternative courses of action.\\r\\nAccountable for accurate and timely completion of work, sharing own expertise with others and guiding less experienced employees.\\r\\nDevelop and maintain business domain knowledge, understanding of the Australian Signals Directorate data, data analysis skills and awareness emerging techniques and trends.\\nJob Description\\r\\n\\r\\nThe Operations Research Scientist (Senior Data Scientist, Airline) will be responsible for designing and implementing advanced optimization models to enhance operational efficiency, reduce costs, and drive strategic decision-making within the Airline.\\r\\n\\r\\nThis is role reports into technology (Analytics) and operations (integrated planning) and focuses on translating complex business challenges into mathematical frameworks and delivering actionable, data-driven solutions.\\r\\n\\r\\nWhat You Will Do\\r\\n\\r\\nDesign and implement optimization models—including mixed-integer programming (MIP) and linear programming (LP)— to tackle complex business problems, enhance resource allocation, and drive key performance outcomes.\\r\\nUtilize Data Science and advance mathematical modelling techniques to continuously refine models using real-world data, feedback loops, and performance metrics. \\r\\nConduct data-driven analyses and develop predictive and prescriptive models to support structured, scalable solutions for operational challenges.\\r\\nProduce clear technical documentation and executive summaries that communicate optimization results, insights, key trends, and strategic recommendations.\\r\\nCollaborate cross-functionally with software developers, data engineers, data scientists, and consultants in an agile setup, ensuring high-quality, process-compliant delivery.\\r\\nAct as a trusted advisor by building strong relationships with stakeholders and leadership teams, enhancing their understanding of data science applications.\\r\\nPlay a pivotal role in fostering a data-driven, customer-centric decision-making culture across Jetstar.\\r\\nDevelop deep domain expertise in key operational areas and ensure optimization models align with real-world business constraints and objectives.\\r\\nConduct research to evaluate and recommend advanced optimization techniques, solvers, and tools based on applicability, scalability, and integration needs.\\r\\nAct as a strategic link between technical teams and business stakeholders, translating complex models into actionable insights within the broader solution landscape.\\r\\n\\r\\nWhat You Will Bring\\r\\n\\r\\nTertiary degree (or equivalent experience) in a highly quantitative discipline (e.g. operations research, mathematics, computer science, or related science field.)\\r\\nMinimum three years’ experience in competitive consumer retail markets such as energy, financial services, insurance, telecommunications or airlines\\r\\nExperience in delivery of optimization solutions to solve complex business problems and drive data-driven decision-making.\\r\\nDemonstrated expertise in developing and implementing advanced optimization models (LP, MIP and optimisation specific heuristics) to solve intricate, real-world challenges.\\r\\nExperienced with optimization solvers like Gurobi, CPLEX, HiGHS, or other industry-standard solvers.\\r\\nStrong background in data analysis and predictive modelling, enabling the creation of scalable, data-driven solutions.\\r\\nProficient in Python and SQL for building models, conducting analysis, and automating processes.\\r\\nSkilled in transforming complex technical concepts into clear, actionable insights, with strong stakeholder management expertise.\\nAbout the job\\r\\nJoin Australia's leading online retailer of furniture and homewares!\\r\\nHybrid working arrangement - work from home and our vibrant St Peters Office\\r\\nBe part of a thriving business with an amazing culture and fantastic benefits!\\r\\n\\r\\nTemple & Webster is Australia’s number one online retailer of furniture and homewares, with a vision to make the world more beautiful, one room at a time. To realise our vision we need a Senior Data Scientist with exceptional analytical skills, demonstrated collaboration and expertise in SQL and Python to elevate our data-driven strategies.\\r\\n\\r\\nJoin us in making the world more beautiful one room at a time!\\r\\n\\r\\nWhat we’re looking for\\r\\nThe main responsibilities of this role include:\\r\\n\\r\\nIndependently lead data projects to boost customer engagement metrics and revenue\\r\\nIdentify data science opportunities, , formulate, execute, and manage data science projects across the business\\r\\nWork on all stages of data science solutions including data preprocessing, feature engineering, model development, and evaluation, with clear communication of insights\\r\\nDevelop AI/ML models working with cross functional teams/stakeholders for internal decision making (Category Management, Marketing, Buying etc)\\r\\nCreate customer-facing features/tools to enhance engagement and sales\\r\\nOversee model productionisation and guide maintenance/monitoring set-up with the development team\\r\\nCollaborate with data and AI teams to refine solutions and standards\\r\\nCoach teams on effective model use and value creation\\r\\nUtilise AI tools to enhance solution effectiveness\\r\\nLeverage AI resources within Temple & Webster's ecosystem\\r\\nBe the go-to data science expert in the business\\r\\n\\r\\n\\r\\nWhat you already have\\r\\nMinimum 7 years of Data Science or Machine Learning experience in deriving actionable insights from data\\r\\nTertiary qualification in a quantitative discipline (e.g., Statistics, Mathematics, Computer Science)\\r\\nExpert level proficiency in SQL and Python, with strong skills in statistical analysis and machine learning\\r\\nAbility to apply sophisticated mathematical and statistical techniques to solve complex business problems\\r\\nDeep expertise in developing, evaluating predictive models, and extracting valuable insights from large datasets using supervised, unsupervised learning, deep learning, etc\\r\\nStrong mathematical and statistical foundation including linear algebra, multivariable calculus, probability theory, hypothesis testing, maximum likelihood, and optimisation among others\\r\\neCommerce experience is a bonus\\r\\nThorough understanding of the full model lifecycle and experimental design\\r\\nProven analytical and problem-solving skills with commercial acumen\\r\\nAbility to manage multiple tasks and prioritise effectively for different stakeholders\\r\\nExperience with both structured and unstructured data\\r\\nStrong stakeholder management and communication skills\\r\\nAbility to work in a fast-paced environment with a curious mindset\\r\\nDo work that matters:\\r\\n\\r\\nAs we continue our pursuit of NPS +30 we need to transform the way we engage with our customers and colleagues. As the Senior Staff Data Scientist you will be responsible for the development and delivery of next generation Gen AI Chatbots and AI models supporting the bank to level-up AI adoption. This role involves thought leadership, AI solution design, and designing and developing cutting edge AI models to uplift customer experience.\\r\\n\\r\\nAs the Senior Staff Data Scientist, you will play a pivotal role in ensuring CBA continues to be a trusted leader in the Responsible use of AI. You are passionate about all things related to Data Science, Gen AI and Machine Learning tools, highly technical and enjoy being fully hands-on with coding in Python and R. You will be driving projects and outcomes, being heavily focused on design and implementation of models and monitoring.\\r\\n\\r\\nYou will come with strong business acumen along with demonstrable experience in using statistics and machine learning techniques and will be part of a high-performing team of data scientists that concentrate on building customized data science models to solve business problems.\\r\\n\\r\\nSee yourself in our team:\\r\\n\\r\\nAs a community, our data scientists have diverse backgrounds and industry experience with state-of-the-art machine learning research and development expertise, and in-depth knowledge of structured and unstructured data in the Bank.\\r\\n\\r\\nWe are open-minded, curious and passionate about problem solving and data driven innovation at scale. In CBA we collaborate with universities and start-ups, hold regular knowledge sharing, technical training and external meet-ups which aid in growing our data science community and industry reputation.\\r\\n\\r\\nWe support our people with the flexibility to balance where work is done with at least half your time each month connecting in office. We also have many other flexible working options available including changing start and finish times, part-time arrangements and job share to name a few. Talk to us about how these arrangements might work for you.\\r\\n\\r\\nWe are seeking people who are:\\r\\n\\r\\nPassionate about building market leading commercial, Gen AI Chatbot, machine learning systems that utilise state of the art Gen AI Solutions.\\r\\nExcited about tackling new challenging business problems in Data Science using Gen AI tools.\\r\\nAdept with using a combination of commercial and technical Data Science models to find unique solutions in adding strategic value to the business.\\r\\nCollaborative in nature to work with product teams, curious enough to solve data problems and courageous in positively influencing senior stakeholders.\\r\\nNatural leaders that enjoy a collaborative environment and want to help foster a positive and inclusive culture that promotes development and growth.\\r\\nConstantly thinking outside the box and breaking boundaries to solve complex data science problems.\\r\\n\\r\\nWe are also interested in hearing from people who:\\r\\n\\r\\nCan provide technical and thought leadership on emerging Gen AI technologies and the impact they might have towards the strategic objectives of the business stakeholders\\r\\nAre interested to coach and nurture junior members of the Data Science team\\r\\nCan build and develop ML and AI solution and utilise statistical models to drive business decisions and outcomes\\r\\nCan develop and implement Gen AI solutions in collaboration with business representatives and other technical teams, including MLOps teams, Solution Architects and product owners to provide implementable AI and ML solutions and insights to drive engagement and customer satisfaction\\r\\nAre experienced in working with large and complex data within a corporate environment, and automating processes\\r\\nHave a natural talent to communicate and positively influence a diverse and large group of stakeholders and cross-functional teams\\r\\n\\r\\nTech skills:\\r\\n\\r\\nWe use a broad range of tools, languages, and frameworks. We don’t expect you to know them all but experience or exposure to any Data Science, AI, Gen AI and Machine Learning systems and statistical modelling skills will set you up for success in this role!\\r\\n\\r\\nSolid foundations in statistical modelling, Gen AI, Data Science and ML models.\\r\\nProficient in developing and deploying a variety of AI / data science solutions including generative AI, NLP, reinforcement learning.\\r\\nCore background in Software Engineering and Data Science streams.\\r\\nHighly experienced in coding to build and implement the ML models easily and quickly using Python, R and PySpark.\\r\\nStrong leadership skills to handle the end-to-end process- build, implement and deploy Gen AI and ML models, and communicate timelines and key milestone to stakeholders and product owners.\\r\\nBachelors in Computer Science or Engineering or Mathematics or Statistics or similar disciplines\\r\\n\\r\\nWorking with us:\\r\\n\\r\\nWhether you’re passionate about customer service, driven by data, or called by creativity, a career with CommBank is for you.\\r\\n\\r\\nOur people bring their diverse backgrounds and unique perspectives to build a respectful, inclusive, and flexible workplace. One where we’re driven by our values, and supported to share ideas, initiatives, and energy. One where making a positive impact for customers, communities and each other is part of our every day.\\r\\n\\r\\nHere, you’ll thrive. You’ll be supported when faced with challenges and empowered to tackle new opportunities. You’ll be empowered to do your best work and be given the choice on when and where that work happens. We really love working here, and we think you will too.\\r\\n\\r\\nDon’t miss out on this opportunity and APPLY TODAY!\\nAbout the job   One of our Government clients is seeking to engage Senior Data Scientists for Canberra, Sydney, Brisbane, Hobart and Melbourne  locationsPlease note Candidate must have Baseline security clearance Initial contract duration-12 MonthsExtension term-6 monthsLocation of work-ACT, NSW, QLD, TAS, VICWorking arrangements-HybridKey Responsibilities      Utilising advanced statistical and machine learning methods to:Analyse complex datasets to deliver actionable insights that inform policy and operational decisionsPerform exploratory data analysis to identify trends, patterns, and insightsDesign, build, and deploy machine learning modelsDevelop model monitoring methods to evaluate model performance and fine-tune algorithms for optimal resultsProvide recommendations for process improvements based on data findingsWorking closely with stakeholders to:Ensure data science projects are effectively integrated into larger government initiativesFacilitate workshops and meetings to gather requirementsCollaborate with cross-functional teams to understand business requirementProvide regular updates and progress reports to stakeholdersIdentify opportunities for cross-departmental collaborationConveying technical results and findings to:Clearly and effectively communicate technical results to non-technical audiences, such as senior executives, stakeholders, and community groupsUse reports, presentations, and data visualisation tools to present findingsContributing to team efforts to:Represent the team in forums and meetingsShare knowledge and expertise with team membersSupport team efforts as requiredFoster a collaborative and inclusive team environmentIntroducing new analytical and data science methods to:Systematically embed new analytical and data science methods into compliance processesBuild capability within the teamStay updated with the latest advancements in data science and analyticsEvaluate the effectiveness of new methods and refine as neededIdentifying, developing, and maintaining relationships to:Develop and maintain relationships to deliver work outcomesIdentify potential collaborators and establish partnerships Key Capabilities      Experience delivering professional data science solutions in an operational environmentExperience in programming for AI and Machine LearningExperience with SQL, R/Python, and R Shiny programmingHands on experience with EDW and cloud environments, ideally GCPExcellent at managing diverse stakeholder relationships to advance and achieve business objectivesProficient in statistical analysisCapable of developing innovative data-driven solutions using advanced analytics methodologiesProven ability to set priorities, monitor progress and ensure the successful delivery of functions and programsCapable of meeting tight deadlines efficientlyAbility to communicate data analysis to non-technical staff and managersAbility to promote data literacy through effective communication of data that is aligned with work processesExcellent interpersonal skills with the ability to collaborate effectively with cross-functional teams and engage with a diverse range of internal and external stakeholdersIdeally, have knowledge of the Medicare Benefits Schedule or the Pharmaceutical Benefits Scheme Essential criteria1. Demonstrated experience with data analytical platforms such as R Shiny, SAS, Power BI, Qlik.2. Demonstrated experience in programming for AI and Machine Learning.3. Demonstrated experience with EDW and cloud environments, ideally GCP.4. Demonstrated experience with SQL, and R or Python for programming.5. Demonstrated experience delivering professional data science solutions in an operational environment.6. Demonstrated ability to collaborate effectively within team and across various areas of an organisations, manage diverse stakeholder relationships, and communicate data analysis to non-technical stakeholders.If you would like to apply for the above role, then please send your updated resume with Cover Letter to rahul.sharma@italliance.com.au                                         \\n\\nAbout the job  Permanent Full Time OpportunityYou are determined to stay ahead of the latest technologies in Data Science!We're one of the largest and most advanced Data Science practices in the countryTogether we can build state-of-the-art Data Science models that power seamless experiences for millions of customers. See yourself in the team:In Financial Services (FS) we support the Group’s strategy of building tomorrow’s bank today for our customers, through a focus on three key priorities of trust, resilience, and capital generation. The FS Analytics Crew supports Financial Services through process optimization and automation by leveraging data and use of a range of analytical tools including AI. We enable business decision-making and insights by providing automated, timely and accurate data and reporting. We proactively identify opportunities to drive world class technology solutions for strategic decision making.The Chief Data and Analytics Office (CDAO) Practice delivers the groups data and AI solutions to ensure the highest levels of customer service through excellent insights, process excellence and AI innovation. Our team is composed of data scientists, engineers, and technology leaders, who bring in the right mix of skills to enable this transformation. We also work very closely with our business and operations colleagues to support these services which are critical to the Australian and Global economy.Do work that mattersAs a Data Scientist , you will use technical knowledge and understanding of business domain to define, own, drive and deliver highly complex data science projects independently. You will be involved in and drive multiple initiatives in parallel.You will work closely with key stakeholders, and cross-functional teams within the CDAO and the FS Analytics teams and the wider business stakeholders to ensure best customer and business outcomes, while solving real-time business problems. As a community, our data scientists have diverse backgrounds and industry experience, with state-of-the-art machine learning research and development expertise, and in-depth knowledge of structured and unstructured data in the Bank.Role and Responsibilities:      Lead and drive AI/ data science projects independently, owning the end-to-end process from data collection to model deployment.Apply Gen AI, NLP, Computer Vison, Deep Learning, advanced statistical and ML predictive modelling techniques to develop and implement scalable AI solutions, ensuring accuracy and reliability of models.Collaborate with business and cross-functional teams to understand their needs and implement AI driven solutions.Opportunity understanding/ problem formulation and suggest AI solutions to help the business for maximum impact. Mentor junior data scientists and provide guidance on their projects.Present complex results to non-technical stakeholders, translating data insights into actionable business strategies.Develop and deploy production-grade AI/ machine learning models in cloud-based and on-prem platforms.Lead cross-functional teams in the design and execution of AI/ data science projects, ensuring alignment with business objectives.Stay abreast of emerging technologies and industry trends, continuously enhancing expertise in AI/ data science methodologies and tools.Drive innovation by exploring new approaches and techniques for solving complex business problems through data analysis and AI/ ML modelling.Strongly support the adoption of AI/ data science across the organization.Contribute to the strategic planning and direction of AI/ data science initiatives within the organisation.Engineering: working in a multidisciplinary squad alongside architect and data engineer We want to hear from you if you have:      Demonstrated experience in data science or statistical modelling working on end-to-end projects, preferably within retail banking or financial services.Advanced technical skills - ability to generate actionable value-add insights by understanding complex data and using tools such as Python, R, Spark, Teradata SQL, SAS and experience in working with Big Data Platforms, e.g. Hadoop/HDFS, Omnia and etc, using Hive and/or SparkKnowledge of AI, Deep Learning, Natural Language Processing (NLP), Computer Vision, Large Language Models (LLM), Generative AI (e.g., Open-source models & GTP’s, RAG, Guardrails for fairness / privacy)Knowledge of H2O.ai, GitHub, Big Data and ML EngineeringA relevant tertiary qualification e.g. Mathematics, Statistics, Economics, Econometrics, Computer Science or EngineeringExperience in a risk or audit domain would be beneficial. About You:      You enjoy tackling exciting and challenging business problems.You have an innovation mindset blended with pragmatic thinkingYou use a combination of commercial thinking and analytically driven results to formulate new ideas for the business.You’re driven and love to work with others to solve problems and provide the best possible solutions for your stakeholders.You enjoy a collaborative team environment and want to help foster a positive and inclusive culture that promotes development and growth Working with us:Whether you’re passionate about customer service, driven by data, or called by creativity, a career here is for you. At CommBank, we advocate and facilitate a culture of inclusion and respect, celebrating all cultures, abilities, genders, expressions of gender and sexual orientation. Read more about our commitment to inclusion and diversity on our website.Our people bring their diverse backgrounds and unique perspectives to build a respectful, inclusive and flexible workplace. We are working hard to recruit people who represent the diversity of our customers and our society. If you're excited about this opportunity but you don't meet every single requirement, or your experience doesn't align perfectly, we still want to encourage you to send in your application. You may just be the perfect candidate for this opportunity or another within CommBank.At CommBank we will inspire you with work that makes a difference, surround you with talented people that respect and value each other, and empower you to grow professionally and personally. Most of all, making a positive impact for customers, communities and each other is part of our every day.We’re determined to make a real difference for Australia’s first peoples. We encourage all interested applicants to apply. If you’re already part of the Commonwealth Bank Group (including BankWest), you’ll need to apply through Sidekick to submit a valid application. We’re keen to support you with the next step in your career                                         \\n\\nAbout the job  Purpose & Primary ObjectivesKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applicationsKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.Ability to dive deep into problem areas of Machine Learning and look for innovative technology solutions to not only advance the current state of solution, but also to generate new options that can provide significant strategic advantageMinimum 4 years of experience in developing and deploying Machine Learning, Deep Learning, NLP solutionsKey ResponsibilitiesUnderstand business process/ business problems and device Machine Learning solutions to optimize business processes wherever possible.Formulating machine learning solutions to business metrics, designing features from the data available from many large data sources, training, evaluating, and deploying models in productionDeveloping high-performance algorithms for precision targeting, testing and implementing these algorithms in scalable, production-ready code; Interacting with other teams to define interfaces and understanding and resolving dependenciesTechnical SkillsBachelors or higher computational science with an emphasis in Machine LearningMinimum 4 years of experience of writing production quality code.Experience with traditional as well as modern statistical techniques, including Regression, Support Vector Machines, Regularization, Boosting, Random Forests, and other Ensemble MethodsExperience with end-to-end modelling projects emerging from research effortsExperience in creating large data feature stores for Machine learning in Teradata or similar databases.                                         \\r\\n\\nAbout the job  At Stomble we're an exciting startup looking for a highly experienced Data Scientist to join our team. We are a small startup bootstrapping our way forward with a group of eager engineers and designers already with a high level of ambition to build and deliver our MVP solution to the world.Stomble is building a data analytics infrastructure that is powered by AI to help companies centralize their data to make it more accessible across their organization whilst eliminating decision fatigue. Our mission and vision are to use innovative solution to increase economic opportunity for businesses globally.We're looking for a skilled Data Scientist who has experience in leading a new project from scratch, can help understand the companies needs and has the capability to motivate team members to deliver outstanding results.Key Responsibilities:      Collect, clean, and analyze small and large datasets from various sources.Build predictive models and machine learning algorithms to solve business problems.Communicate findings and insights clearly to stakeholders through data visualizations and storytelling.Work with data engineers to define data pipelines and data architecture requirements.Collaborate with product teams to identify and prioritize analytics opportunities.Monitor model performance and conduct regular reviews to ensure accuracy over time.Solutions or alternatives to help train the data based on use-cases.  Technical Requirements:      Has worked with structured, unstructured and SQL.Understanding and has experience using Python, R, Bash and Shell scripting.Has experience doing supervised and unsupervised learning e.g., regression, classification and clustering.Model evaluation and validation techniques e.g., cross-validation, ROC-AUC.Time series analysis. Writing, constructing and iterating upon algorithmsUnderstanding of Azure infrastructure e.g., Data factory & ML Studio.Understanding of CI/CD continuous integration and deployment.Natural Language Processing is optional.Reinforcement learning is optional The successful candidate will have:      Intrinsic motivations to learn and continuously fill in any gaps where required. Openness and willingness to think outside the box to solve and find solutions.Willingness to work in different disciplines where needed or required.Able to connect the big picture, create an action plan and work their way backwards to a workable goal. Limitations of the position:      Position is fully remote.This role is unpaid due to the company not generating any revenue or raising any funding.We are unable to sponsor applicants at this time.Must be located in Australia. If this sounds like an opportunity, you'd like to be apart of, we'd like to hear from you!                                         \\r\\n\\nAbout the job     DescriptionData Scientist      Accrue up to an extra 12 days of leave per year through our Life Days program.Work with world leading technology business at the forefront of innovation.We’re a ‘Family Friendly’ certified workplace – we understand the often many and varied roles our team members need to play within their own unique family setting and actively support them. Our team feel Leidos is a great place to work. Learn more about our culture and benefits by visiting us here https://www.leidos.com/company/global/australia/careersDo Work That MattersLeidos Australia delivers IT and airborne solutions that protect and advance the Australian way of life. Our 2000 local experts, backed by our global experience and network of partners, are working to solve the world’s toughest challenges in government, intelligence, defence, aviation, border protection and health markets.Your New Role And ResponsibilitiesThis role will be responsible for the design, development, build, installation, testing and evaluation of data science solutions that utilize Big Data technologies.      Conduct comprehensive analysis of user requirements and develop tailored analytical models.Perform rigorous testing and validation of machine learning and AI models.Execute advanced data preparation, blending, and in-depth data analysis.Create compelling data visualizations and generate informative reports.Deploy and maintain data analytics solutions with emphasis on performance and security optimization. What You’ll Bring To Make An Impact      Extensive experience in engineering or project environments with proven expertise in data mining, analysis, model development, and visualization.Expert-level knowledge of diverse data technologies, including analytics architectures, data management, and advanced tools for structured and unstructured data.Strong hands-on development skills with proficiency in multiple programming languages and technologies (SQL, R, Python, Java, Spark, TensorFlow, Power BI).Demonstrated capability in developing ML/AI and Large Language Models (LLMs) models with an innovative, research-driven approach.Comprehensive technical expertise spanning data science principles, AI/ML tools, and, specifically application of LLMs within production environments. Don’t worry if you don’t tick all the boxes – if you meet most of them, we encourage you to submit your application. We’re most interested in your strengths, what you want to learn and how far you want to go.Due to the nature of work we do for our customers, all applicants need to hold an existing NV-1 security clearance and be capable of upgrading to a NV-2 security clearance if required.Diverse Team Members, Shared Values and a Common PurposeProviding our customers with smarter solutions takes an incredible team with diversity of thought, experience and perspectives driving innovation. Inclusion is at the heart of our culture and is one of our core values. It's about creating a workplace where everyone can do important work, feels welcome, valued, and respected, and has equal access to opportunities to thrive. Paul Chase – Chief Executive, Leidos Australia.Leidos Australia is an equal opportunities organisation and is committed to creating a truly inclusive workplace. We welcome and encourage applications from Aboriginal and Torres Strait Islanders, culturally and linguistically diverse people, people with disabilities, veterans, neurodiverse people, and people of all genders, sexualities, and age groups.Our five Advocacy Groups (Women and Allies Network,\\u202fYoung Professionals, Defence & Emergency Services, Action for Accessibility and Abilities and Pride+) provide an opportunity for team members to connect and collaborate on shared interests, and work to support and celebrate our diverse community.Next Steps      To apply for this role, follow the links or apply via our Careers page.Recruitment process – 1-2 interviews (depending on seniority of role) & background checks.Applicants may also need to meet International Traffic in Arms Regulations (ITAR) requirements. In certain circumstances this can place limitations on persons who hold dual nationality, permanent residency or are former nationals of certain countries as per ITAR 126.1.We are committed to making our recruitment process accessible to all candidates. Please contact our Careers team careers.au@au.leidos.com if you’d like to discuss any additional support during your application or throughout the recruitment process. Original PostingFor U.S. Positions: While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.Pay RangeThe Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.                                         \\r\\n\\nWe’re Hiring: Data Scientist - Computer Vision | Melbourne CBD | AI InnovationReady to make an impact with AI? One of our premier clients is seeking a  Data Scientist - Computer Vision to join their high-performing innovation team in the heart of Melbourne. If you’re passionate about harnessing AI to solve real-world problems and thrive in a collaborative, cutting-edge environment – this one’s for you.About the RoleYou’ll play a key role in developing AI-powered solutions to analyse a massive backlog of asset imagery to assess their condition and recommend maintenance, repair, or replacement. The goal? Smarter, faster, and more cost-effective asset management.What You’ll Be Doing      Using AWS (including Bedrock) and Python to build image analysis pipelinesCreating machine learning models for visual recognition and predictive maintenanceTranslating image data into actionable insights for decision-makersCollaborating within a well-funded, forward-thinking innovation hubOnce you have crushed this project, there is a backlog of AI initiatives that are just waiting to be delivered Tech You’ll Use      AWS Cloud Services (including Bedrock)Python (with relevant AI/ML libraries)Cutting-edge tools in computer vision and deep learning LocationMelbourne CBD – this is an on-site role. Local candidates only, please.Why Join?This is more than a job – it’s a chance to shape the future of asset intelligence. You’ll be surrounded by brilliant minds, supported with strong funding, and empowered to experiment, learn, and innovate every day.                 \\r\\n\\nJob DescriptionThe OpportunityWe Are Seeking a Talented Data Scientist To Join Our Established And Dynamic Team, Where Growth Opportunities Abound. As An Integral Member, You'll Delve Into Hands-on Development And Improvement Of AWS Cloud Platforms. Here's What You'll Do      Improve and extend our scalable cloud platform while facilitating the storage, processing and retrieval of data such as point clouds and imagery at the petabyte scale; Work alongside our data scientists to deploy algorithms and machine learning at scale;Engage key technical stakeholders to build requirements for future development;Maintain the integrity and high availability of our platform and cloud services.  Who Are WeWe are Fugro, the world’s leading Geo-data specialist, focused on mapping, modeling and monitoring data about the earth’s surface and the structures built upon it. With clients spanning land, sea, and space, our respected reputation is grounded in valuing our employees as our greatest asset, a steadfast commitment to delivery excellence, and an innovative approach that leverages cutting-edge technology to solve our clients' challenges.This role forms part of our R&D team, part of our Innovation department with a strong focus on automating complex spatial data analysis workflows, and providing information to clients via intuitive web-based interfaces. Real world data (LIDAR point clouds, imagery, sonar scan and multi-beam) is captured remotely by vessels, autonomous underwater vehicles, planes or helicopters. Data are uploaded to AWS and distilled to provide clients insights via the use of modern machine learning, scalable cloud computing, and 3D visualisation approaches.How We WorkWe embrace a modern and collaborative work culture, where everyone feels comfortable to be themselves; all voices heard, all cultures respected.Fugro is committed to helping our employees move towards their full potential, through alignment, autonomy, and personal accountability, in a supportive, information sharing work environment. While we support remote work, we also value face-to-face interactions, whiteboard discussions and sharing a meal together.About YouTo succeed in this role, you will need to       Bachelor’s degree in relevant field;Creative and critical thinker with excellent communication abilities; Knowledge of and practical experience with applying advanced machine learning techniques (ideally in a production environment); Robust statistical skills and a passion for learning and improving skills;Sound understanding of software engineering and proficiency with one or more technical computing languages such as Julia, Python, MATLAB, R, Haskell.  Extra Things We Value      Have 1-3 years’ experience writing software in a commercial or research environment;Solid experience in numerical simulation/high-performance computing/parallelisationExperience developing in a cloud computing environment (AWS preferred) and signal processing; Proficiency in one or more of the following languages C++, C#, Java, JS, Typescript;Exposure to LiDAR, point clouds, imagery, geodesy and/or positioning systems;A Master’s or PhD in a technical field such as Mathematics, Physics, Computer Science/Engineering, or alternatively equivalent research experience in the industry.  Fugro offers      Enjoy flexible working hours, including hybrid work-from-home options, to support your work-life balance and personal commitmentsBe rewarded with competitive compensation and annual salary reviewsFeel the security that our salary continuance insurance provides in case of unforeseen circumstancesOur global career framework and Fugro academy provides pathways to support your growth and career progressionOur inclusive, information sharing culture means that colleagues happily share knowledge, setting each other and our teams up for successBe part of a diverse, friendly and supportive environment, where you can feel free to be yourselfOur modern office is close to the South Bank train station and Cultural Center busway station, also served by the CityGlider bus serviceEnjoy the ability to be fully present with your family upon the arrival of a new child, with our 12 week Parental Leave policyProgressive company with industry new and cutting-edge technology  How To ApplyPlease ensure your CV is up to date, with relevant information clear and succinct and follow the application prompts.Candidates will be asked to provide evidence of Australian working rights to be considered for this position (visa sponsorship is not offered).Fugro reserves the right to close this advert at any time.Applications or interest via recruitment agencies will not be accepted at this time.Disclaimer For Recruitment AgenciesFugro does not accept any unsolicited applications from recruitment agencies. Acquisition to Fugro Recruitment or any Fugro employee is not appreciated.                 \\r\\n\\nAbout the job      Job DescriptionThe OpportunityWe Are Seeking a Talented Data Scientist To Join Our Established And Dynamic Team, Where Growth Opportunities Abound. As An Integral Member, You'll Delve Into Hands-on Development And Improvement Of AWS Cloud Platforms. Here's What You'll Do      Improve and extend our scalable cloud platform while facilitating the storage, processing and retrieval of data such as point clouds and imagery at the petabyte scale; Work alongside our data scientists to deploy algorithms and machine learning at scale;Engage key technical stakeholders to build requirements for future development;Maintain the integrity and high availability of our platform and cloud services.  Who Are WeWe are Fugro, the world’s leading Geo-data specialist, focused on mapping, modeling and monitoring data about the earth’s surface and the structures built upon it. With clients spanning land, sea, and space, our respected reputation is grounded in valuing our employees as our greatest asset, a steadfast commitment to delivery excellence, and an innovative approach that leverages cutting-edge technology to solve our clients' challenges.This role forms part of our R&D team, part of our Innovation department with a strong focus on automating complex spatial data analysis workflows, and providing information to clients via intuitive web-based interfaces. Real world data (LIDAR point clouds, imagery, sonar scan and multi-beam) is captured remotely by vessels, autonomous underwater vehicles, planes or helicopters. Data are uploaded to AWS and distilled to provide clients insights via the use of modern machine learning, scalable cloud computing, and 3D visualisation approaches.How We WorkWe embrace a modern and collaborative work culture, where everyone feels comfortable to be themselves; all voices heard, all cultures respected.Fugro is committed to helping our employees move towards their full potential, through alignment, autonomy, and personal accountability, in a supportive, information sharing work environment. While we support remote work, we also value face-to-face interactions, whiteboard discussions and sharing a meal together.About YouTo succeed in this role, you will need to       Bachelor’s degree in relevant field;Creative and critical thinker with excellent communication abilities; Knowledge of and practical experience with applying advanced machine learning techniques (ideally in a production environment); Robust statistical skills and a passion for learning and improving skills;Sound understanding of software engineering and proficiency with one or more technical computing languages such as Julia, Python, MATLAB, R, Haskell.  Extra Things We Value      Have 1-3 years’ experience writing software in a commercial or research environment;Solid experience in numerical simulation/high-performance computing/parallelisationExperience developing in a cloud computing environment (AWS preferred) and signal processing; Proficiency in one or more of the following languages C++, C#, Java, JS, Typescript;Exposure to LiDAR, point clouds, imagery, geodesy and/or positioning systems;A Master’s or PhD in a technical field such as Mathematics, Physics, Computer Science/Engineering, or alternatively equivalent research experience in the industry.  Fugro offers      Enjoy flexible working hours, including hybrid work-from-home options, to support your work-life balance and personal commitmentsBe rewarded with competitive compensation and annual salary reviewsFeel the security that our salary continuance insurance provides in case of unforeseen circumstancesOur global career framework and Fugro academy provides pathways to support your growth and career progressionOur inclusive, information sharing culture means that colleagues happily share knowledge, setting each other and our teams up for successBe part of a diverse, friendly and supportive environment, where you can feel free to be yourselfOur modern office is close to the South Bank train station and Cultural Center busway station, also served by the CityGlider bus serviceEnjoy the ability to be fully present with your family upon the arrival of a new child, with our 12 week Parental Leave policyProgressive company with industry new and cutting-edge technology  How To ApplyPlease ensure your CV is up to date, with relevant information clear and succinct and follow the application prompts.Candidates will be asked to provide evidence of Australian working rights to be considered for this position (visa sponsorship is not offered).Fugro reserves the right to close this advert at any time.Applications or interest via recruitment agencies will not be accepted at this time.Disclaimer For Recruitment AgenciesFugro does not accept any unsolicited applications from recruitment agencies. Acquisition to Fugro Recruitment or any Fugro employee is not appreciated.                                         \\r\\n\\nAbout the job\\r\\nProsper is seeking a Sr. Data Scientist for the Data Science team under our Credit Risk Analytics vertical. You will become a core contributor with machine learning expertise on the credit risk team and deliver results that directly impact our business.\\r\\n\\r\\nSenior DS - MS with 6+ years of experience or PhD with 3+ years of experience\\r\\n\\r\\nProblems You Will Solve\\r\\n\\r\\nBuild industry-leading machine learning models for managing credit and fraud risks\\r\\nLeverage multiple complex data sources such as credit bureau reports and customer supplied information at large scale to optimize approve/decline and credit line assignment decisions\\r\\nCollaborate with engineers to deploy your models into a production environment\\r\\nPropose and execute solutions to various problems within business constraints\\r\\nUse responsible AI technique following regulatory requirements and lending best practices\\r\\nHelp the team with developing tools and workflow solutions to increase data science productivity\\r\\nActively monitor the credit risk models in production\\r\\nExtract the most value out of data to significantly impact our key business metrics\\r\\nAssess the potential usefulness and validity of machine learning algorithms and features through various data sources\\r\\nConduct ad-hoc analysis related to risk management, investor services, operations and corporate development\\r\\n\\r\\nAbout You\\r\\n\\r\\n6+ years of work experience in fintech, finance, or other high impact field applying statistical and machine learning predictive techniques (educational experience taken into consideration). Consumer lending experience in unsecured personal loan or credit card is a plus\\r\\nAdvanced degree (M.S./PhD) preferably in statistics, computer science, engineering, physical sciences, economics, or related technical field\\r\\nExpert knowledge in one of the statistical programming languages such as Python, and database languages such as SQL\\r\\nSolid understanding of coding best practices and model documentation\\r\\nStrong communication skills – ability to clearly and succinctly communicate technical subject matter to other team members and senior management\\r\\nStrong interpersonal skills – collaborate with people across functions and develop strong relationships\\r\\nAbility to work unsupervised in fast-paced environment and prioritize among parallel projects\\r\\nAbility to think within regulatory guidelines with a mindset toward reproducible research\\r\\nSelf-motivated, results-oriented, enthusiastic, and a creative thinker\\r\\n\\r\\nThe salary range for a Senior Data Scientist position is $151,000 - $200,000 annually, plus bonus and generous benefits. In determining your salary, we will consider your experience, location, and other job-related factors.\\r\\n\\r\\n#IND1\\r\\n\\r\\nAbout Us\\r\\n\\r\\nFounded in 2005 as the first peer-to-peer marketplace lending platform in the U.S., Prosper was built on a simple idea: connect people who want to borrow money with those who want to invest. Since inception, Prosper has helped more than 2 million people gain access to affordable credit with over $27 billion in loans originated through its platform. Our mission is to help our customers advance their financial well-being through a variety of products including personal loans, credit, home equity lines of credit (HELOC), and our newest product, HELoan. Our diverse culture rewards accountability and cross functional teamwork because we believe this encourages innovative thinking and helps us deliver on our mission.\\r\\n\\r\\nWe’re on a mission to hire the very best, and we are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere. It is important to us that every hire connects with our vision, mission, and core values. Join a leading fintech company that’s democratizing finance for all!\\r\\n\\r\\nOur Values\\r\\n\\r\\nDiversity expands opportunities\\r\\n\\r\\nCollaboration creates better solutions \\r\\n\\r\\nCuriosity fuels our innovation\\r\\n\\r\\nIntegrity defines all our relationships\\r\\n\\r\\nExcellence leads to longevity \\r\\n\\r\\nSimplicity guides our user experience \\r\\n\\r\\nAccountability at all levels drives results\\nAbout the job   Senior Data Scientist – APACWill you like to join a leader in legal insights and technology?Are you passionate about innovation?About Our Company LexisNexis Legal & Professional, a division of RELX, is a global leader in providing information-based analytics and decision tools for professional and business customers. With a presence in over 150 countries and a workforce of 11,300 employees worldwide, we are committed to delivering exceptional service and innovative solutions.About Our TeamOur team, based in the APAC region, plays a crucial role in supporting all regional functions through comprehensive reporting and data-driven insights. We are currently undergoing an exciting transition, where we are enhancing our data capabilities and embracing Machine Learning and Predictive Analytics to better support our business objectives and drive growth.About The RoleSenior Data Scientists are recognized as subject matter experts within their domain. They are responsible for defining complex data projects and overseeing their execution. While they may lead small program teams, their primary leadership is through influence. They excel in mentoring teams, fostering a culture of collaboration and innovation. Their focus is on developing and implementing best practices and methodologies, ensuring high standards of quality and driving the team's success through their exceptional leadership skills. The ideal candidate will have a strong background in machine learning, predictive analytics, statistical analysis, and data visualization.Responsibilities      Collaborate with cross-functional teams to identify data-driven opportunities and provide actionable insights and data enrichment from internal and external sources. Lead the development and implementation of advanced data models and algorithms to solve complex business problems with Machine Learning and Predictive Analytics. Design and execute experiments to test hypotheses and validate models. Understand data pipelines, connectors, and ETL processes to ensure data quality and integrity. Create detailed documentation related to the experiment processes, including frameworks for problem definition, data preparation, model selection, hyperparameter tuning, model deployment, and presenting results. Communicate findings and recommendations to stakeholders through clear and compelling visualisations and reports. Provide mentorship and guidance to team members to enhance skills across the group.  Requirements      Master’s degree in data science, Statistics, Computer Science, or a related field.7+ years of experience in data science or a related role.Expert and independent in relevant Data Science, Data Mining, and/or Statistical frameworks:      Time Series AnalysisLogistic RegressionClustering and SegmentationStatistical Inference, Analysis of Variance, Principal Component Analysis, Correlation and other core concepts.Basic NLP and Text Analytics - Text Pre-Processing, Topic Modelling, Sentiment Analysis, and Classification Techniques.Data visualization Possess domain expertise in Data Science and/or Statistical Analysis, with the capability to develop advanced models and collaborate with cross-functional teams to deploy them into production.Ability to design, scope, and implement new statistical methodologies with guidance and approval from leadership.Can create and use novel data steps to prepare and process data for analysis.Excels in teaching and directing others with a supportive and caring demeanor.Able to oversee, direct, and encourage others throughout the ML ecosystem.Can create and utilise novel code to conduct analysis, overseeing others in the application of developing the code.Expert in standard coding languages of Python and SQL.Familiarity with Apache Spark/Scala, Unix/Bash, R libraries is a plus. Platform / ML Framework / Tools      Extensive use of DatabricksExtensive use of AWS / Redshift / SagemakerPython libraries central to ML (Scikit-learn, NumPy, Pandas, etc)Knowledge of TensorFlow or PyTorchFamiliarity with Apache Spark ML, ML Flow, PySparkStrong knowledge in Power BI / Tableau Work in a way that works for youWe promote a healthy work/life balance across the organisation. We offer an appealing working prospect for our people. With numerous wellbeing initiatives, shared parental leave, study assistance and sabbaticals, we will help you meet your immediate responsibilities and your long-term goals.Working for youBenefitsWe know that your wellbeing and happiness are key to a long and successful career. These are some of the benefits we are delighted to offer:      Flexible working arrangements Benefits for you and your family Access to learning and development resources  Your recruiter will advise you on the full benefits package for your locationAbout The BusinessLexisNexis Legal & Professional® provides legal, regulatory, and business information and analytics that help customers increase their productivity, improve decision-making, achieve better outcomes, and advance the rule of law around the world. As a digital pioneer, the company was the first to bring legal and business information online with its Lexis® and Nexis® services.LexisNexis, a division of RELX, is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. We are committed to providing a fair and accessible hiring process. If you have a disability or other need that requires accommodation or adjustment, please let us know by completing our Applicant Request Support Form: https://forms.office.com/r/eVgFxjLmAK , or please contact 1-855-833-5120.Please read our Candidate Privacy Policy.                                         \\n\\nAbout the job\\r\\nSenior Data Scientist – APAC\\r\\n\\r\\nWill you like to join a leader in legal insights and technology?\\r\\n\\r\\nAre you passionate about innovation?\\r\\n\\r\\nAbout Our Company\\r\\n\\r\\n \\r\\n\\r\\nLexisNexis Legal & Professional, a division of RELX, is a global leader in providing information-based analytics and decision tools for professional and business customers. With a presence in over 150 countries and a workforce of 11,300 employees worldwide, we are committed to delivering exceptional service and innovative solutions.\\r\\n\\r\\nAbout Our Team\\r\\n\\r\\nOur team, based in the APAC region, plays a crucial role in supporting all regional functions through comprehensive reporting and data-driven insights. We are currently undergoing an exciting transition, where we are enhancing our data capabilities and embracing Machine Learning and Predictive Analytics to better support our business objectives and drive growth.\\r\\n\\r\\nAbout The Role\\r\\n\\r\\nSenior Data Scientists are recognized as subject matter experts within their domain. They are responsible for defining complex data projects and overseeing their execution. While they may lead small program teams, their primary leadership is through influence. They excel in mentoring teams, fostering a culture of collaboration and innovation. Their focus is on developing and implementing best practices and methodologies, ensuring high standards of quality and driving the team's success through their exceptional leadership skills. The ideal candidate will have a strong background in machine learning, predictive analytics, statistical analysis, and data visualization.\\r\\n\\r\\nResponsibilities\\r\\n\\r\\nCollaborate with cross-functional teams to identify data-driven opportunities and provide actionable insights and data enrichment from internal and external sources. \\r\\nLead the development and implementation of advanced data models and algorithms to solve complex business problems with Machine Learning and Predictive Analytics. \\r\\nDesign and execute experiments to test hypotheses and validate models. \\r\\nUnderstand data pipelines, connectors, and ETL processes to ensure data quality and integrity. \\r\\nCreate detailed documentation related to the experiment processes, including frameworks for problem definition, data preparation, model selection, hyperparameter tuning, model deployment, and presenting results. \\r\\nCommunicate findings and recommendations to stakeholders through clear and compelling visualisations and reports. \\r\\nProvide mentorship and guidance to team members to enhance skills across the group. \\r\\n\\r\\nRequirements\\r\\n\\r\\nMaster’s degree in data science, Statistics, Computer Science, or a related field.\\r\\n7+ years of experience in data science or a related role.\\r\\nExpert and independent in relevant Data Science, Data Mining, and/or Statistical frameworks:\\r\\nTime Series Analysis\\r\\nLogistic Regression\\r\\nClustering and Segmentation\\r\\nStatistical Inference, Analysis of Variance, Principal Component Analysis, Correlation and other core concepts.\\r\\nBasic NLP and Text Analytics - Text Pre-Processing, Topic Modelling, Sentiment Analysis, and Classification Techniques.\\r\\nData visualization\\r\\nPossess domain expertise in Data Science and/or Statistical Analysis, with the capability to develop advanced models and collaborate with cross-functional teams to deploy them into production.\\r\\nAbility to design, scope, and implement new statistical methodologies with guidance and approval from leadership.\\r\\nCan create and use novel data steps to prepare and process data for analysis.\\r\\nExcels in teaching and directing others with a supportive and caring demeanor.\\r\\nAble to oversee, direct, and encourage others throughout the ML ecosystem.\\r\\nCan create and utilise novel code to conduct analysis, overseeing others in the application of developing the code.\\r\\nExpert in standard coding languages of Python and SQL.\\r\\nFamiliarity with Apache Spark/Scala, Unix/Bash, R libraries is a plus.\\r\\nPlatform / ML Framework / Tools\\r\\n\\r\\nExtensive use of Databricks\\r\\nExtensive use of AWS / Redshift / Sagemaker\\r\\nPython libraries central to ML (Scikit-learn, NumPy, Pandas, etc)\\r\\nKnowledge of TensorFlow or PyTorch\\r\\nFamiliarity with Apache Spark ML, ML Flow, PySpark\\r\\nStrong knowledge in Power BI / Tableau\\r\\n\\r\\nWork in a way that works for you\\r\\n\\r\\nWe promote a healthy work/life balance across the organisation. We offer an appealing working prospect for our people. With numerous wellbeing initiatives, shared parental leave, study assistance and sabbaticals, we will help you meet your immediate responsibilities and your long-term goals.\\nAbout the job IntroductionA career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.You'll work with visionaries across multiple industries to improve the hybrid cloud and AI journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.Curiosity and a constant quest for knowledge serve as the foundation to success in IBM Consulting.In your role, you'll be encouraged to challenge the norm, investigate ideas outside of your role, and come up with creative solutions resulting in ground breaking impact for a wide network of clients. Our culture of evolution and empathy centers on long-term career growth and development opportunities in an environment that embraces your unique skills and experience.Your Role And Responsibilities      Participar ativamente da implementação de projetos de otimização em diversas áreas do negócio, garantindo que os modelos atendam aos requisitos de eficiência e eficácia;Colaborar com stakeholders de diversas áreas, traduzindo desafios de negócios em problemas matemáticos e propondo soluções viáveis;Analisar grandes volumes de dados e gerar insights acionáveis que orientem a tomada de decisão estratégica da empresa;Documentar e comunicar os resultados das análises de forma clara e eficiente para públicos técnicos e não técnicos;Mentorar cientistas de dados juniores e colaborar com outras áreas de ciência de dados e TI para garantir a melhor performance dos modelos. Preferred EducationBachelor's DegreeRequired Technical And Professional Expertise      Experiência com Algorítimo de Otimização;Conhecimentos de Estatística: estatística descritiva, diagnóstica, distribuições probabilísticas, testes de hipótese e Machine Learning;Conhecimento de programação em Python (bibliotecas como Pandas, Scikit-Learn, Stats Models, PySpark, Keras, PyTorch, TensorFlow);Experiência com manipulação de dados: interação com bancos de dados e manipulação de dados usando a linguagem SQL;Conhecimento em visualização de dados, e versionamento de código (Git/Bitbucket);Experiência em Cloud (Azure, AWS, Google Cloud, IBM Cloud).  Preferred Technical And Professional Experience      Será considerado um diferencial a vivência/experiência em alguma iniciativa de liderança: projeto ou equipe);Formação superior em qualquer curso relacionado a tecnologia da Informação: Engenharia da Computação, Ciência da Computação, Estatística, Matemática ou áreas correlatas.                                          \\r\\n\\nAbout the job\\r\\nAt Dataiku, we're not just adapting to the AI revolution, we're leading it. Since our beginning in Paris in 2013, we've been pioneering the future of AI with a platform that makes data actionable and accessible. With over 1,000 teammates across 25 countries and backed by a renowned set of investors, we're the architects of Everyday AI, enabling data experts and domain experts to work together to build AI into their daily operations, from advanced analytics to Generative AI.\\r\\n\\r\\nThe role of a Data Scientist at Dataiku is quite unique. Our Data Scientists not only code up solutions to real-world problems but also participate in client-facing endeavors throughout the customer journey. This includes supporting their discovery of the platform, helping integrate Dataiku with other tools and technologies, some user training, and co-developing data science projects from design to deployment. Just as the non-technical skills are important, so too are the technical. Our Data Scientists work on the Dataiku platform every day. Aside from the visual tools, our team uses mostly Python and SQL, with occasional work in other languages (e.g., R, Pyspark, JavaScript, etc.). An ideal candidate is excited to learn complex new technologies and modeling techniques while being able to explain their work to other data scientists and clients.\\r\\n\\r\\nKey Areas Of Responsibility (What You’ll Do)\\r\\n\\r\\nScope and co-develop production-level data science projects with our customers across different industries and use cases \\r\\nHelp users discover and master the Dataiku platform via user training, office hours, and ongoing consultative support \\r\\nProvide strategic input to the customer and account teams that help make our customers successful. \\r\\nLead Data Scientist long-term engagements: You will coordinate agile sprints, prioritize tasks, estimate effort, do backlog grooming \\r\\nProvide data science expertise both to customers and internally to Dataiku’s sales and marketing teams \\r\\nLead technical data science projects pre-sales scoping and design appealing proposals \\r\\nFlag technical and non-technical account risks (onboarding issues, performance pitfalls, timeline slippage) \\r\\nNavigate customer conflict and de-escalation, in particular on churn-risk engagements \\r\\nDevelop custom Python-based “plugins” in collaboration with Solutions, R&D, and Product teams, to enhance Dataiku’s functionality \\r\\nRun demo booth/tech talk duties at company public events (e.g. Everyday AI) \\r\\nContribute to internal assets (internal best practice or external blog post/project on the public gallery) \\r\\nMentor junior team members \\r\\n\\r\\nExperience (What We’re Looking For)\\r\\n\\r\\nCuriosity and a desire to learn new topics and skills \\r\\nEmpathy for others and an eagerness to share your knowledge and expertise with your colleagues, Dataiku’s customers, and the general public \\r\\nThe ability to clearly explain complex topics to technical as well as non-technical audiences \\r\\nOver 8 years of experience in advanced analytics, data science projects, or consulting \\r\\nOver 5 years of experience with Python and SQL \\r\\nOver 5 years of experience with building ML models and using ML tools (e.g. sklearn, tensorflow, pytorch) \\r\\nExperience with Consulting and/or Customer-facing Data Science roles \\r\\nExperience with LLM \\r\\nExperience with data visualization and building web apps with Python frameworks (Dash, Streamlit) \\r\\nUnderstanding of underlying data systems such as Cloud architectures and SQL \\r\\nTravel to the client site \\r\\n\\r\\nBonus Points For Any Of These\\r\\n\\r\\nExperience using Dataiku DSS \\r\\nExperience working in the Financial Services Industry \\r\\nExperience with Data Engineering or MLOps \\r\\nExperience developing WebApps in Javascript \\r\\nExperience building APIs \\r\\nPassion for teaching or public speaking \\nAbout the job\\r\\nData Scientist – Marketing Science \\r\\n\\r\\n\\r\\n\\r\\nWhy us?: \\r\\n\\r\\nAnnalect is a global data, technology, and analytics consultancy, created over 10 years ago, with offices across Australia and major global hubs. We create solutions across data, measurement, analytics, visualisation, and marketing technology to maximise the effectiveness of media and our client's media ROI.\\r\\n\\r\\n\\r\\nAnnalect is building creative solutions around data, technology, and analytics to help our clients navigate the increasingly complicated and fast changing world of media.\\r\\n\\r\\n\\r\\nSome of our benefits include:\\r\\nPaid subscription to Coursera\\r\\nRegular training courses via MFA, Internal training programs & Industry events\\r\\nFlexible Working Hours & WFH\\r\\nEmployee Value Program\\r\\nMonthly Employee Recognition & Rewards\\r\\nSocial Club, Regular team events and Summer Half Days.\\r\\n\\r\\n\\r\\nYour Purpose: \\r\\n\\r\\nMarketing has a long history of using data to drive decisions through techniques such as econometric modelling, experiment design, customer segmentation and market research. The digitisation of marketing has produced new ways to advertise as well as orders of magnitude more data that can be used to drive decisions.\\r\\n\\r\\n\\r\\nAs a Data Scientist at Annalect Australia, you will be responsible for analysing large amounts of raw information to find patterns that will help improve our clients’ offerings. Your goal will be to help our clients analyse trends to make better business / customer decisions.\\r\\n\\r\\n\\r\\nYour Responsibilities: \\r\\n\\r\\nApply your broad skillset across data and analytics to understand business problems, create insights and envisage practical solutions in areas such as media measurement and customer insights\\r\\nBuild bespoke digital attribution models and Marketing Mix Models (MM) and improve our product offerings\\r\\nProvide business solutions and optimisation through various statistical and quantitative methods\\r\\nProvide statistical models to support predictive analytics and deliver non-technical presentations to all levels of the business as well as technical documentation to the wider team\\r\\nDerive insights from data and communicate those insights to a non-technical audience through presentations and documentation\\r\\n\\r\\n\\r\\nYour Expertise: \\r\\n\\r\\nHigh attention to detail, a spirit of curiosity and a drive to consistently make things better through innovation or efficiency.\\r\\nSound understanding of Media Technology (Ad Servers, DMPs, DSPs, CDPs, CRM, AI)\\r\\nUnderstanding of Data Visualisation and BI solutions and Dashboarding software (Tableau, Power BI, Data Studio, Datorama)\\r\\nExcellent communication skills - being able to both interpret and convey information in a clear, concise way with people from technical and non-technical background and use data to tell a story\\r\\nStrong understanding of media, and media dat\\n\"],\n",
       " 'Data Engineer': [\"Role: AWS Cloud Data EngineerType: Permanent Location: SydneySkills: ETL, AWS, Redshift, SQL, PythonExperience: 8+ years Role Description:+Looking for highly skilled AWS Data Platform Engineers with extensive experience in Banking, Financial Services (BFS), and stock trading. The ideal candidates will be responsible for designing, implementing, and maintaining robust data platforms on AWS, ensuring seamless integration and real-time data processing for financial applications.Key Responsibilities:      Design and Development: Architect and develop scalable data platforms using AWS services such as Redshift, Glue, Athena, EMR, and S3.Data Integration: Integrate various data sources, including real-time stock trading data, into the AWS data platform.ETL Processes: Develop and manage ETL processes to ensure efficient data ingestion, transformation, and loading.Data Security: Implement security best practices to protect sensitive financial data, including encryption and access controls.Performance Optimization: Optimize data storage and retrieval processes to ensure high performance and low latency.Monitoring and Maintenance: Monitor data platform performance, troubleshoot issues, and perform regular maintenance.Collaboration: Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver solutions.Documentation: Maintain comprehensive documentation of data architecture, processes, and configurations. Qualifications:      Education: Bachelor's or Master's degree in Computer Science, Engineering, or related field.Experience: Minimum of 10 years of experience in data engineering, with at least 5 years on AWS. Technical Skills:      Proficiency in AWS services (Redshift, Glue, Athena, EMR, S3).Strong knowledge of data warehousing and big data technologies.Experience with real-time data processing using Kafka or similar technologies.Expertise in SQL, Python, and other scripting languages.                  \\r\\n\\nhe Role Responsible and accountable for the existing and upcoming data engineering project/s.Key Responsibilities      Leads one or more moderately complex data engineering projects, playing a key role in designing and building data pipelines that drive faster, data-informed decision-making across the business.Co-develops and implements scalable big data solutions that handle high volume and velocity, ensuring readiness for future growth.Stays abreast of the latest development, testing, and deployment techniques to accelerate the delivery of new data pipelines and integration of diverse data sources.Guides teams through Agile methodologies and associated tools, coaching direct reports where applicable.Proactively partners with the business to mature their MI and analytics platforms using the ESSA approach, with opinions actively sought by stakeholders.Engages in strategic discussions and project meetings to influence adoption of analytical insights.Utilizes strong business partnering skills to gather, model, and analyse complex data and performance metrics.Operates as a Data Engineering Lead, independently developing and deploying cutting-edge big data platforms to support advanced analytics and enterprise-wide data processing. Skills & Experience      Flexibility as this role will be working with team based Brisbane office and other locations with different time zone.Proficiency in developing front-end and back-end applications using .Net and Python.Proficiency in relational database and queries.Good understanding of Microsoft Azure ecosystem.Experience in working on containerized application using Kubernetes.Good interpersonal skill to interact with peers and business users.Experience working in Databricks (Preferred).Experience working in Energy/Financial trading (or similar) industry (Preferred). Only candidates with the right to work in Australia will be considered (Australian Citizen, PR holder, or valid Australian working visa).What You Need To Do NowIf you’re interested in this role, click 'apply now' to forward an up-to-date copy of your CV. If this job isn’t quite right for you but you are looking for a new position, please contact us for a confidential discussion about your career.Caryl Isuan | caryl.isuan@airswift.com | +61 8 6318 1100About UsAirswift is an international workforce solutions provider with 1,000 employees and 9,000 contractors operating in over 70 countries. Our expertise across the energy, mining, infrastructure, and technology sectors is unparalleled.By joining Airswift's community, you gain exclusive access to our candidate portal, 'Airswift Digital,' where you can receive early notifications of upcoming assignments, manage your timesheets digitally, and access exclusive content designed to support your career development.At Airswift, we are passionate about providing equal employment opportunities and embracing diversity to benefit all. We actively encourage applications from any background                 \\r\\n\\nUrgent requirement of AWS Data Engineer - Perm/ Contract - Sydney  Requirements       Minimum of 10 years of experience in data engineering, with at least 5 years on AWS Looking for highly skilled AWS Data Platform Engineers with extensive experience in Banking, Financial Services (BFS), and stock trading Must have experience in designing, implementing, and maintaining robust data platforms on AWS Design and Development - Architect and develop scalable data platforms using AWS services such as Redshift, Glue, Athena, EMR, and S3 Data Integration - Integrate various data sources, including real-time stock trading data, into the AWS data platform ETL Processes Develop and manage ETL processes to ensure efficient data ingestion, transformation, and loading Data Security Implement security best practices to protect sensitive financial data, including encryption and access controls Performance Optimization Optimize data storage and retrieval processes to ensure high performance and low latency Monitoring and Maintenance Monitor data platform performance, troubleshoot issues, and perform regular maintenance Documentation Maintain comprehensive documentation of data architecture, processes, and configurations Proficiency in AWS services (Redshift, Glue, Athena, EMR, S3) Strong knowledge of data warehousing and big data technologies Should have experience with real-time data processing using Kafka or similar technologies Should have expertise in SQL, Python, and other scripting languages Very Good Communication Skills            Duration: Permanent / 6 Months and possible extension    Eligibility: Australian/NZ Citizens/PR Holders only    Email: jobs@hasthasolutions.com                                                   Desired Skills and Experience                                    Minimum of 10 years of experience in data engineering, with at least 5 years on AWS Looking for highly skilled AWS Data Platform Engineers with extensive experience in Banking, Financial Services (BFS), and stock trading Must have experience in designing, implementing, and maintaining robust data platforms on AWS Design and Development - Architect and develop scalable data platforms using AWS services such as Redshift, Glue, Athena, EMR, and S3 Data Integration - Integrate various data sources, including real-time stock trading data, into the AWS data platform ETL Processes Develop and manage ETL processes to ensure efficient data ingestion, transformation, and loading Data Security Implement security best practices to protect sensitive financial data, including encryption and access controls Performance Optimization Optimize data storage and retrieval processes to ensure high performance and low latency Monitoring and Maintenance Monitor data platform performance, troubleshoot issues, and perform regular maintenance Documentation Maintain comprehensive documentation of data architecture, processes, and configurations Proficiency in AWS services (Redshift, Glue, Athena, EMR, S3) Strong knowledge of data warehousing and big data technologies Should have experience with real-time data processing using Kafka or similar technologies Should have expertise in SQL, Python, and other scripting languages Very Good Communication Skills Duration: Permanent / 6 Months and possible extension Eligibility: Australian/NZ Citizens/PR Holders only Email: jobs@hasthasolutions.com                              \\r\\n\\nLet’s Talk About You\\r\\n\\r\\nYou are a forward-thinking professional with a passion for designing data pipelines and developing data models. Your expertise in building modern enterprise data platforms, including Data Lakes, data warehouses, and operational data stores, is essential. You excel in developing solutions for data acquisition, storage, aggregation, provision, and consumption, ensuring they are fit for both current and future needs.\\r\\n\\r\\nExperience with Microsoft Azure Platform and Snowflake Cloud Data Platform.\\r\\nStrong knowledge of SQL, Python, database design, integration design, and architecture governance.\\r\\nProven ability to solve problems, think creatively, and work both unassisted and as part of a team.\\r\\nQuick to learn and adapt to changing situations.\\r\\n\\r\\n\\r\\nHow will you support our “Why?”\\r\\n\\r\\nIn this critical role, you will design and build data pipelines and models to deliver a modern enterprise data platform. You will work with various teams to assess data requirements and provide solutions aligned with enterprise data architecture and security standards.\\r\\n\\r\\nDevelop, enhance, and maintain existing ETL pipelines.\\r\\nMonitor, improve, and remediate any issues with Azure Data Factory and Snowflake data pipelines.\\r\\nManage, maintain, and enhance the enterprise data platform.\\r\\nDefine and provide best practices, set architectural standards, and design guidelines.\\r\\nEvaluate and plan data implementation requirements and solution designs.\\r\\nEnsure all key stakeholders and data domains are appropriately represented in the solution design and build process.\\r\\nCreate documentation on data warehouse structure and data pipelines.\\r\\nActively improve processes within the Data insights platform.\\r\\nFollow data engineering best practices.\\r\\nCollaborate with agile teams, vendors, and business stakeholders to ensure clarity of data requirements and appropriateness of design and solution.\\r\\nWork closely with the Data Governance team to align with the data governance framework.\\r\\nBuild SnapLogic pipeline to ingest data from variety of sources such as blob, API, Netsuite, Salesforce etc.\\nRole DescriptionThis is a contract role for a Data Engineer at Gamma Data. As a Data Engineer, you will be responsible for managing and supporting the dbt + Lakehouse installation, developing and implementing cloud infrastructure solutions, and working closely with software development teams. This is a hybrid role, with the majority of the work being located in Melbourne, VIC, but some remote work is also acceptable.Experience      Software Development and Infrastructure skillsExperience in Cloud ComputingData Engineering backgroundStrong problem-solving and analytical skills Rate$900/day - $1200/day\\nAbout the job  Adactin, one of the fastest-growing companies in Australia and the APAC region, thrives on its solutions and services on INNOVATION. Our vision, combined with our experience in the market, has continuously allowed us to grow our expertise which is visible in our comprehensive portfolio. We innovate, strategize, consult, and operate for organizations to achieve their business objectives through our services in Software development, Testing services, Digital transformation, Enterprise solutions, and quality ICT training. Adactin is always committed to delivering with conviction and belief in our people. Seize the opportunity and advance your career in a dynamic environment to augment your growth and success.We are primarily seeking a Senior Data Engineer to work with our team.      Visa: Full Work RightsLocation: Sydney only Roles and Responsibilities :      Develop and maintain efficient, scalable data pipelines from diverse internal and external sources into large cloud environments using AWS Serverless technologies.Implement data processing and transformation solutions using Snowflake, and SQL for large-scale operations.Automate manual processes and streamline data delivery pipelines, leveraging Informatica for ETL and re-engineering infrastructure for performance.Design and manage infrastructure as code to support ELT processes into MS SQL-based data warehouses and cloud-based data lakes.Integrate Data and Sagemaker for scalable data processing, transformation, and advanced analytics workflows.Support and manage ETL/ELT processes, including data cleansing and transformation using Informatica and Snowflake.Build tools and scripts to support Analytics, BI, and Data Science teams in building optimized products, including dashboards in Qlik.Perform root cause analysis on data pipelines and sources to address complex business questions and improve reliability.Identify opportunities for process improvements, including data transformation, metadata handling, dependency tracking, and workload management.Apply deep expertise in SQL, RDBMS design, and data flow analysis to troubleshoot and optimize data solutions.Ensure secure, virtualized cloud environments with embedded Data Governance practices across the data lifecycle.Prioritize automation, scalability, and data governance in all data solutions using tools like Informatica, and AWS.                                          \\r\\n\\nAbout the job     Billigence Pty Ltd is a specialist in the delivery of market-leading Business Intelligence and CRM solutions. Headquartered in Sydney, Australia and with offices in Prague, London, Frankfurt and Singapore our passion is data and our focus is the delivery of end-to-end solutions via a talented team of skilled professionals.We are partners with leading edge software platforms including Tableau, Alteryx, Collibra, Snowflake, GCP and Salesforce.What We Are Looking ForWe are looking for Senior Data Engineers to join our team and work alongside a leading Australian organisation.Initial 6 month contract + extensions. Sydney based, hybrid with 2 days per week in the Sydney office.Skills/experience Required      7-10+ Years data engineering experience4+ Years in-depth Python programming experience (Pandas, Polars data engineering in Python)Object oriented and functional paradigms experience5+ Years in-depth SQL experience (design patterns and strategies for processing data)Minimum 1-2 years commercial docker experienceMinimum 1 years experience with TerraformCICD - 1 or more of the following tools (i.e. GitHub Actions, Azure Pipelines or GCP Workflows)Commercial experience using cloud technologies i.e Google Cloud Platform (GCP), Azure or AWSHighly desirable - Experience with BigQuery, Airflow, Cloud composer etc.Excellent written and verbal communication skills - essential If this sounds like something you are interested in, please apply with your most up-to-date CV and we will be in touch!Please note: Only successful applicants will be contacted.\\nABOUT US Billigence Pty Ltd is a specialist in the delivery of market-leading Business Intelligence and CRM solutions. Headquartered in Sydney, Australia and with offices in Prague, London, Frankfurt and Singapore our passion is data and our focus is the delivery of end-to-end solutions via a talented team of skilled professionals.We are partners with leading edge software platforms including Tableau, Alteryx, Collibra, Snowflake, GCP and Salesforce.What we are looking for:We are looking for Senior Data Engineers to join our team and work alongside a leading Australian organisation.Initial 6 month contract + extensions. Sydney based, hybrid with 2 days per week in the office.Skills/experience required:      7-10+ Years data engineering experience4+ Years in-depth Python programming experienceObject oriented and functional paradigms experience5+ Years in-depth SQL experience (design patterns and strategies for processing data)Minimum 1-2 years commercial docker experienceMinimum 1 years experience with Terraform CICD experienceCommercial experience using cloud technologies i.e Google Cloud Platform (GCP), Azure or AWSExcellent written and verbal communication skills - essential If this sounds like something you are interested in, please apply with your most up-to-date CV and we will be in touch!Please note: Only successful applicants will be contacted.                 \\r\\n\\n About the job   We are looking for a Senior Data Engineer to design, implement and support telemetry systems and solutions within CDC's IT Environments. The incumbent will act as a steward for capabilities development and will be responsible for improving processes and setting up ETL between source systems and reporting tools which utilise real time data collected from our Data Centre Monitoring Systems.Key responsibilities include: • Design, implement and support complex data processing ETL solutions.• Design, implement and support data pipelines and architectures to efficiently move data from various sources to storage and processing systems.• Implement effective monitoring for data systems, identifying bottlenecks and optimizing for scalability. • Build distributed, highly available MQTT Brokers across multiple regions.• Perform mapping and categorization of data using T-SQL.• Ensure data accuracy, consistency, and completeness through validation and other quality measures.• Implement and support DevOps automation solutions.• Develop and maintain professional relationships with key internal and external stakeholders.• Ensure the compliance of IT and OT solutions with CDC's design guidelines, and all relevant policies and standards.• Participate in incident resolution as required for telemetry solution and related systems.About you: • Experience in the use of relational and non-relational database technologies such as SQL Server,• Experience in the deployment, configuration and management of virtualized infrastructure and systems.• Experience with availability and performance monitoring tools and techniques.• Experience with scripting / programming, preferably using Bourne/Bash, Powershell, C# and Python.• Experience with MQTT, Modbus or Bacnet protocols OR OPC-UA is a bonus!Essential:• Australian Citizenship and the ability to obtain and maintain AGSVA Security Clearance at Negative Vetting 1 level. How to Apply:If you are looking to own your career and take on a new challenge in a fast-growing Australian Technology company and be part of our amazing team, please submit your resume and cover letter by clicking Apply.                                         \\r\\n\\nAbout the role:\\r\\n\\r\\nWe are looking for a Senior Data Engineer to design, implement and support telemetry systems and solutions within CDC's IT Environments. The incumbent will act as a steward for capabilities development and will be responsible for improving processes and setting up ETL between source systems and reporting tools which utilise real time data collected from our Data Centre Monitoring Systems.\\r\\n\\r\\nKey responsibilities include: \\r\\n\\r\\n• Design, implement and support complex data processing ETL solutions.\\r\\n• Design, implement and support data pipelines and architectures to efficiently move data from various sources to storage and processing systems.\\r\\n• Implement effective monitoring for data systems, identifying bottlenecks and optimizing for scalability. \\r\\n\\r\\n• Build distributed, highly available MQTT Brokers across multiple regions.\\r\\n• Perform mapping and categorization of data using T-SQL.\\r\\n• Ensure data accuracy, consistency, and completeness through validation and other quality measures.\\r\\n• Implement and support DevOps automation solutions.\\r\\n• Develop and maintain professional relationships with key internal and external stakeholders.\\r\\n• Ensure the compliance of IT and OT solutions with CDC's design guidelines, and all relevant policies and standards.\\r\\n• Participate in incident resolution as required for telemetry solution and related systems.\\r\\n\\r\\nAbout you: \\r\\n\\r\\n• Experience in the use of relational and non-relational database technologies such as SQL Server,\\r\\n• Experience in the deployment, configuration and management of virtualized infrastructure and systems.\\r\\n• Experience with availability and performance monitoring tools and techniques.\\r\\n• Experience with scripting / programming, preferably using Bourne/Bash, Powershell, C# and Python.\\r\\n• Experience with MQTT, Modbus or Bacnet protocols OR OPC-UA is a bonus!\\r\\n\\r\\nEssential:\\r\\n\\r\\n• Australian Citizenship and the ability to obtain and maintain AGSVA Security Clearance at Negative Vetting 1 level.\\nAbout the role:\\r\\n\\r\\nData Engineer \\r\\n\\r\\nData, Engineering, Science or IT degree.\\r\\n5+years working in data, IT, digital, or customer experience working in a similar industry (i.e. telecommunications or information technology)\\r\\n 5+ years working in Agile \\r\\n Prior experience in data management, data analytics or digital data related fields\\r\\n Proven experience working with, and managing multiple stakeholders \\r\\n Proven experience with delivery of products within agile\\r\\n Strong customer focus and passionate about customer experience \\r\\n Strong analytical, written and oral communication skills \\r\\n Effective validation of IT solutions and identification of alternative solutions for the delivery of products to ensure data governance and data management requirements are met\\r\\n Highly organized, with attention to detail\\r\\n Demonstrated ability to meet strict deadlines\\r\\n Fluency in the English language – high level of written and strong comprehensive skills\\nAs Data Engineer, you will:\\r\\n\\r\\nDevelop and support: Build and maintain a data lake of source data, including extracting data from a wide variety of non-standard sources and updating data at irregular schedules.\\r\\nAutomation: Automate data extractions, loads and transformations to update data and documentation efficiently.\\r\\nData integration: Design, implement, and optimise data warehouses, ensuring data is transformed into a user-friendly format. This includes advanced SQL querying and warehouse design.\\r\\nTechnical expertise: Provide expert support in areas including business intelligence, reporting, and data analysis, leveraging technologies such as Metabase, Qlik Sense, PowerBI, and R.\\r\\nAI and workflows: Utilise AI to improve workflows and automate repetitive tasks, enhancing overall efficiency.\\r\\nDocumentation: Develop, publish, and maintain detailed architectural and procedural documentation for core data capabilities and functions.\\r\\nVersion control: Implement version control using Git to ensure a robust and traceable development process.\\r\\nCollaborative efforts: Work closely with cross-functional teams to gather requirements and ensure data solutions meet business and client expectations.\\r\\n\\r\\n\\r\\nA bit about you: \\r\\n\\r\\nExperience: 3-5 years of experience in data engineering, particularly with the Azure Cloud platform including Azure Data Factory, Microsoft Fabric and data build tool (DBT) for transformation\\r\\nTechnical skills: Proficiency in building and maintaining data lakes, advanced SQL query development, data warehouse design, and automation of data transformations.\\r\\nAnalytical skills: Strong analytical and problem-solving skills, with a proven ability to diagnose and identify root causes of complex data issues.\\r\\nCommunication: Excellent oral and written communication skills, able to document processes clearly and interact effectively with various stakeholders.\\r\\nAgile development: Experience working with Agile development principles.\\r\\nProactivity: A proactive approach to learning new technologies and improving existing processes.\\r\\nR Skills: Experience using R for existing ETL pipelines\\r\\nVersion control: Experience and commitment to using Git for version control.\\r\\nAI integration: Knowledge of using AI to enhance workflows is highly regarded.\\nAs Data Engineer, you will:\\r\\n\\r\\nDevelop and support: Build and maintain a data lake of source data, including extracting data from a wide variety of non-standard sources and updating data at irregular schedules.\\r\\nAutomation: Automate data extractions, loads and transformations to update data and documentation efficiently.\\r\\nData integration: Design, implement, and optimise data warehouses, ensuring data is transformed into a user-friendly format. This includes advanced SQL querying and warehouse design.\\r\\nTechnical expertise: Provide expert support in areas including business intelligence, reporting, and data analysis, leveraging technologies such as Metabase, Qlik Sense, PowerBI, and R.\\r\\nAI and workflows: Utilise AI to improve workflows and automate repetitive tasks, enhancing overall efficiency.\\r\\nDocumentation: Develop, publish, and maintain detailed architectural and procedural documentation for core data capabilities and functions.\\r\\nVersion control: Implement version control using Git to ensure a robust and traceable development process.\\r\\nCollaborative efforts: Work closely with cross-functional teams to gather requirements and ensure data solutions meet business and client expectations.\\r\\n\\r\\n\\r\\nA bit about you: \\r\\n\\r\\nExperience: 3-5 years of experience in data engineering, particularly with the Azure Cloud platform including Azure Data Factory, Microsoft Fabric and data build tool (DBT) for transformation\\r\\nTechnical skills: Proficiency in building and maintaining data lakes, advanced SQL query development, data warehouse design, and automation of data transformations.\\r\\nAnalytical skills: Strong analytical and problem-solving skills, with a proven ability to diagnose and identify root causes of complex data issues.\\r\\nCommunication: Excellent oral and written communication skills, able to document processes clearly and interact effectively with various stakeholders.\\r\\nAgile development: Experience working with Agile development principles.\\r\\nProactivity: A proactive approach to learning new technologies and improving existing processes.\\r\\nR Skills: Experience using R for existing ETL pipelines\\r\\nVersion control: Experience and commitment to using Git for version control.\\r\\nAI integration: Knowledge of using AI to enhance workflows is highly regarded.\\r\\nAs Data Engineer, you will:\\r\\n\\r\\nDevelop and support: Build and maintain a data lake of source data, including extracting data from a wide variety of non-standard sources and updating data at irregular schedules.\\r\\nAutomation: Automate data extractions, loads and transformations to update data and documentation efficiently.\\r\\nData integration: Design, implement, and optimise data warehouses, ensuring data is transformed into a user-friendly format. This includes advanced SQL querying and warehouse design.\\r\\nTechnical expertise: Provide expert support in areas including business intelligence, reporting, and data analysis, leveraging technologies such as Metabase, Qlik Sense, PowerBI, and R.\\r\\nAI and workflows: Utilise AI to improve workflows and automate repetitive tasks, enhancing overall efficiency.\\r\\nDocumentation: Develop, publish, and maintain detailed architectural and procedural documentation for core data capabilities and functions.\\r\\nVersion control: Implement version control using Git to ensure a robust and traceable development process.\\r\\nCollaborative efforts: Work closely with cross-functional teams to gather requirements and ensure data solutions meet business and client expectations.\\r\\n\\r\\n\\r\\nA bit about you: \\r\\n\\r\\nExperience: 3-5 years of experience in data engineering, particularly with the Azure Cloud platform including Azure Data Factory, Microsoft Fabric and data build tool (DBT) for transformation\\r\\nTechnical skills: Proficiency in building and maintaining data lakes, advanced SQL query development, data warehouse design, and automation of data transformations.\\r\\nAnalytical skills: Strong analytical and problem-solving skills, with a proven ability to diagnose and identify root causes of complex data issues.\\r\\nCommunication: Excellent oral and written communication skills, able to document processes clearly and interact effectively with various stakeholders.\\r\\nAgile development: Experience working with Agile development principles.\\r\\nProactivity: A proactive approach to learning new technologies and improving existing processes.\\r\\nR Skills: Experience using R for existing ETL pipelines\\r\\nVersion control: Experience and commitment to using Git for version control.\\r\\nAI integration: Knowledge of using AI to enhance workflows is highly regarded.\\r\\n\\r\\n\\r\\nSome benefits of working with us\\r\\n\\r\\nBelong to a small, supportive team that cares about your professional development. We offer active career and professional development support, funding professional development activities and reimbursing one professional membership per year.\\r\\nOpportunities to grow with us as we expand into new products and sectors\\r\\nFlexible working arrangements, including WFH options. Hybrid working is supported while acknowledging that the fastest learning happens when working face to face. Staff receive a once off payment to set up your WFH office.\\r\\nA collaborative bonus that reflects company performance is available to all permanent employees.\\r\\nAll staff have great flexibility around public holidays. We understand that people of different cultures, religions, and political beliefs may wish to choose to take alternative days of leave from scheduled holidays.\\r\\nSupporting working parents by providing 18 weeks of paid primary carer parental leave and 10 weeks of paid secondary carer parental leave (access to parental leave is gender neutral).\\nWe are currently seeking an experienced Data Engineer for a contract engagement with a leading consultancy, supporting a high-profile data migration project.\\r\\n \\r\\nKey Responsibilities:\\r\\n\\r\\nIdentify, triage, and resolve defects across data engineering workflows\\r\\n\\r\\nOptimise PySpark code and ETL pipelines to support large-scale data migration\\r\\n\\r\\nCollaborate closely with developers, testers, and business stakeholders to ensure stability and performance across data pipelines\\r\\n\\r\\nUtilise Control-M for scheduling and automation of data workflows\\r\\n\\r\\nSupport ongoing troubleshooting and enhancement of Azure Synapse Analytics solutions\\r\\n\\r\\nTechnical Requirements:\\r\\n\\r\\nAdvanced proficiency in PySpark, Azure Synapse Analytics, Control-M, and data pipeline development\\r\\n\\r\\nStrong SQL skills and a solid understanding of data warehousing and cloud-based data architectures\\r\\n\\r\\nDemonstrated experience in large-scale data migrations\\r\\n\\r\\nPrevious banking industry experience is highly desirable\\nOpportunity snapshot:\\r\\n\\r\\n \\r\\nAn exciting, 12 month fixed term opportunity has become available for a Data Engineer\\r\\n\\r\\n \\r\\n \\r\\nOur Global 3x6 Elephant Strategy set us a clear ambition to be the world's most customer-centric healthcare company. To bring our ambition to life, it is critical we elevate the focus on digital and data, evolving how we do things to create value for our customers now and into the future. \\r\\n\\r\\n \\r\\nAs a Data Engineer at Bupa you will bring specialist skills which enables Bupa ANZ to execute on its Data Strategy. Collectively, we are working to build the future state of Bupa’s data platforms on Microsoft Azure services, and to enable the business to work strategically with data to maximise competitive performance. Data Engineers contribute to the evolution of Bupa’s data estate via squad, project, and business partnering approaches. \\r\\n\\r\\n \\r\\nAt Bupa ANZ, Data Engineers have a broad skill set, and work across various Data Services; Operational Data and Analytics, Data Warehouse and BI, Advanced Analytics and Self-service, and across different business units including Health Insurance, Health Services, and corporate functions.\\r\\n \\r\\nWhat will I be doing?\\r\\n\\r\\n \\r\\nIdentify a variety of solution options and work collaboratively with stakeholders to analyse the pros/cons and implications of each option, decide on the right option to proceed with. \\r\\nEngineer data pipelines from data acquisition and ingestion though the data lake, various means of consumption including, reports, analytics, dashboards, applications, models, and API’s. \\r\\nStrong knowledge using SQL to write and tune complex and optimised queries over large datasets.\\r\\nEngineering of CI/CD processes and execution of releases following source code control and DevOps best practices. \\r\\nEnsure solutions use secure development practices that are well tested including identification and implementation of data quality and integrity checks \\r\\nEvaluate and Spike new technologies and approaches and recommend pathways to adoption. \\r\\n \\r\\nRequirements for the role:\\r\\n\\r\\n \\r\\nEducated to minimum of degree level in engineering, computer science or related technology discipline \\r\\n2 to 5 years’ experience as a software or data engineer \\r\\nAzure certifications(s) or equivalent \\r\\nStrong level SQL/TSQL \\r\\nStrong level Spark \\r\\nStrong CI/CD experience \\r\\nStrong engineering practices \\r\\nMicrosoft Azure data services skills, specifically: \\r\\nAzure Synapse/Azure SQL DW (Useful) \\r\\nAzure DataBricks/Spark (Mandatory) \\r\\nAzure Data Factory (Mandatory) \\r\\nAzure Analysis Services (Useful) \\r\\nAzure ML (Nice to have) \\r\\nAzure Cosmos DB (Useful) \\r\\nAzure EventHubs (Useful) \\r\\nPowerBI (Useful) \\r\\n \\r\\nAbout us:\\r\\n\\r\\n \\r\\nBupa has a strategic goal of being the most customer-centric digital healthcare organisation, with the use of data as an explicit pillar of this strategy. \\r\\n\\r\\n \\r\\nThe program pillars include:\\r\\n\\r\\nA Customer-Centric Focus - Bupa aims to grow our data products, the data that people have access to and the utilisation of data as a strategic asset that drives our Connected Care, Core Modernisation, and other strategic change agents\\r\\nData Access and Democratization - Ensuring everyone has access to and can understand the data Bupa holds; reducing barriers to entry and make the complex simple for people who want to leverage our data assets\\r\\nSupport for Access - Help leaders in ensuring easy access to data for people that they work with\\r\\nEmpowerment - Enabling business people to do their role as owners, stewards, and leaders.\\r\\nDelivery & Customer Value First - Treat data as an asset to create value for customers and remove barriers to achieving compliance, risk, or other barriers to value\\nAbout the job      You are passionate to stay ahead of the latest\\u202fAWS Cloud, and Data Lake technologies.We're one of the largest and most advanced Data Engineering teams in the country.Together we can build state-of-the-art data solutions that power seamless experiences for millions of customers. Do work that matters:As a Data engineer with expertise in software development / programming and a passion for building data-driven solutions, you’re ahead of trends and work at the forefront of AWS Cloud and Data warehouse technologies.Which is why we’re the perfect fit for you. Here, you’ll be part of a team of engineers going above and beyond to improve the standard of digital banking. Using the latest tech to solve our customers’ most complex data-centric problems.To us, data is everything. It is what powers our innovative features and it’s the reason we can provide seamless experiences for millions of customers from app to branch. We are responsible for CommBank’s key analytics capabilities and work to create world-leading capabilities for analytics, information management and decisioning. We are seeking people who are:      Passionate about building next generation data platforms and data pipeline solution across the bank.Enthusiastic, be able to contribute and learn from wider engineering talent in the team.Ready to execute state-of-the-art coding practices, driving high quality outcomes to solve core business objectives and minimise risks.Capable to create both technology blueprints and engineering roadmaps, for a multi-year data transformational journey.Constantly thinking creatively and breaking boundaries to solve complex data problems.  We are also interested in hearing from people who:      Are data enthusiastic in providing the solutions that source data from various enterprise data platform into data lake, using technologies like Scala, Python, PySpark; transform and process the source data to produce data products, transform and egression to other data platforms like SQL Server, Oracle, Teradata and other cloud platforms. Are practiced in building effective and efficient Data Lake frameworks, capabilities, and features, using common programming language (Scala, PySpark or Python), with proper data quality assurance and security controls.Demonstrated experience in creating python /scala functions/libraries and use them for the config – driven pipeline generation and delivering optimised enterprise-wide data ingestion, data integration and data pipeline solutions for Data Lake & warehouse platforms.Are reliable in building group data products or data assets from scratch, by integrating large sets of data derived from hundreds of internal and external sources.Have experience and responsible for data security and data management.Have a natural drive to communicate and coordinate with different internal stakeholders.  Key Skills & Experience:      Experience in AWS data platforms (Glue, Lambda, Redshift, Athena, Kinesis, EMR).Data Processing: Experience in Python, SQL, Spark, Scala and distributed data processing.Data Governance & Security: Exposure to AWS DataZone, Lake Formation, IAM, encryption.Marketplace & APIs: Exposure to AWS Marketplace, API Gateway, and data monetization strategies.Automation & CI/CD: Experience with Terraform, CloudFormation, GitOps, and serverless frameworks.Performance Optimization: Ability to fine-tune ETL jobs, query performance, and cost efficiencies. Nice to Have:      Experience with Snowflake, or Apache Iceberg.Knowledge of data product thinking and data mesh principles. Working with us:Whether you’re passionate about customer service, driven by data, or called by creativity, a career with CommBank is for you.Our people bring their diverse backgrounds and unique perspectives to build a respectful, inclusive, and flexible workplace with a flexibility to work from any of our engineering hubs in Sydney. One where we’re driven by our values, and supported to share ideas, initiatives, and energy. One where making a positive impact for customers, communities and each other is part of our every day.Here, you’ll thrive. You’ll be supported when faced with challenges and empowered to tackle new opportunities. You’ll be empowered to do your best work and be given the choice on when and where that work happens. We really love working here, and we think you will too.If you're already part of the Commonwealth Bank Group (including Bankwest, x15ventures), you'll need to apply through Sidekick to submit a valid application. We’re keen to support you with the next step in your career.We're aware of some accessibility issues on this site, particularly for screen reader users. We want to make finding your dream job as easy as possible, so if you require additional support please contact HR Direct on 1800 989 696.Advertising End Date: 19/05/2025                                         \\n\\nAbout the job\\nWe're Hiring: Technical Enablement Lead Engineer\\n\\n$1,150/day + Super |  Brisbane Preferred |  No Sponsorship\\nJoin a global tech team delivering next-gen cloud data platforms in the resource sector. You'll play a key role in building and leading scalable solutions using Databricks, Python, SQL, Azure, and AWS.\\nLooking for someone who's:\\nTechnically strong \\nExperienced in data engineering & cloud integration\\nConfident in estimation, design, and delivery\\nCurious, adaptable, and proactive in fast-paced environments\\nSkilled in stakeholder management and team coordination\\nTech environment includes:\\nDatabricks | Azure Synapse | Data Factory | AWS EMR | Spark\\nPython | SQL | CI/CD | Agile delivery | SAFe\\nDetails:\\n$1,150/day + Super\\nBrisbane preferred (Hybrid/Flexible)\\nNo sponsorship available\\nDM me or hit Easy Apply to discuss!\\n #DataEngineering #Databricks #CloudJobs #BrisbaneJobs #ContractRole #TechLeadership #Azure #AWS #BigData #Python\\n\"]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def single_job_desciption_list(df_jobs):\n",
    "    job_dict={}\n",
    "\n",
    "    df_da = df_jobs[df_jobs['Job_Title'] == 'Data Analyst']\n",
    "    df_ds = df_jobs[df_jobs['Job_Title'] == 'Data Scientist']\n",
    "    df_de = df_jobs[df_jobs['Job_Title'] == 'Data Engineer']\n",
    "\n",
    "    da_joined_text = '\\n'.join(df_da['Job_Description'].unique().tolist())\n",
    "    job_dict['Data Analyst'] = [da_joined_text]\n",
    "\n",
    "\n",
    "    ds_joined_text = '\\n'.join(df_ds['Job_Description'].unique().tolist())\n",
    "    job_dict['Data Scientist'] = [ds_joined_text]\n",
    "\n",
    "    de_joined_text = '\\n'.join(df_de['Job_Description'].unique().tolist())\n",
    "    job_dict['Data Engineer'] = [de_joined_text]\n",
    "\n",
    "    return job_dict\n",
    "\n",
    "job_dict = single_job_desciption_list(df_jobs)\n",
    "job_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        job_title                             Top 5 Technical Skills  \\\n",
      "0    Data Analyst              SQL, Power BI, Excel, Python, Tableau   \n",
      "1  Data Scientist  Python, Machine Learning, SQL, R, TensorFlow/P...   \n",
      "2   Data Engineer                     SQL, Python, AWS, Azure, Spark   \n",
      "\n",
      "                                   Top 5 Soft Skills  \n",
      "0  Communication, Analytical Thinking, Problem-So...  \n",
      "1  Problem-Solving, Communication, Analytical Thi...  \n",
      "2  Problem-Solving, Collaboration, Communication,...  \n"
     ]
    }
   ],
   "source": [
    "def call_deepseek_api(job_dict):\n",
    "    # combine all descriptions of a job title to a single list\n",
    "    all_descriptions = []\n",
    "    for job_title, descriptions in job_dict.items():\n",
    "        for desc in descriptions:\n",
    "            all_descriptions.append(f\"{job_title}: {desc}\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Here are various job descriptions for multiple data-related roles (job title is before each colon):\n",
    "\n",
    "{chr(10).join(all_descriptions)}\n",
    "\n",
    "Please analyze all above descriptions and GROUP them into only three categories based on their role:\n",
    "1. Data Analyst (all related titles)\n",
    "2. Data Scientist (all related titles)\n",
    "3. Data Engineer (all related titles)\n",
    "\n",
    "For each category, list Top 5 Technical Skills and Top 5 Soft Skills (based on frequency).\n",
    "\n",
    "Return a CSV with columns:\n",
    "job_title, Top 5 Technical Skills, Top 5 Soft Skills\n",
    "\n",
    "No explanation, CSV ONLY.\n",
    "\"\"\"\n",
    "\n",
    "    url = 'https://api.deepseek.com/v1/chat/completions'\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('DEEPSEEK_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"User-Agent\": \"JobClassification/1.0 (Python)\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-chat\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    answer = response.json()['choices'][0]['message']['content']\n",
    "\n",
    "    #only keep the first csv section, drop all other explanation, markdown and notations etc.\n",
    "    lines = answer.strip().splitlines()\n",
    "    header_idx = None\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line.lower().startswith(\"job_title\"):\n",
    "            header_idx = idx\n",
    "            break\n",
    "    if header_idx is not None:\n",
    "        csv_text = '\\n'.join(lines[header_idx:header_idx+4])\n",
    "    else:\n",
    "        csv_text = answer.strip()\n",
    "\n",
    "\n",
    "    df = pd.read_csv(StringIO(csv_text))\n",
    "    return df\n",
    "\n",
    "\n",
    "df_job_analysis = call_deepseek_api(job_dict)\n",
    "print(df_job_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1699869b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>Top 5 Technical Skills</th>\n",
       "      <th>Top 5 Soft Skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>SQL, Power BI, Excel, Python, Tableau</td>\n",
       "      <td>Communication, Analytical Thinking, Problem-So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Python, Machine Learning, SQL, R, TensorFlow/P...</td>\n",
       "      <td>Problem-Solving, Communication, Analytical Thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>SQL, Python, AWS, Azure, Spark</td>\n",
       "      <td>Problem-Solving, Collaboration, Communication,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        job_title                             Top 5 Technical Skills  \\\n",
       "0    Data Analyst              SQL, Power BI, Excel, Python, Tableau   \n",
       "1  Data Scientist  Python, Machine Learning, SQL, R, TensorFlow/P...   \n",
       "2   Data Engineer                     SQL, Python, AWS, Azure, Spark   \n",
       "\n",
       "                                   Top 5 Soft Skills  \n",
       "0  Communication, Analytical Thinking, Problem-So...  \n",
       "1  Problem-Solving, Communication, Analytical Thi...  \n",
       "2  Problem-Solving, Collaboration, Communication,...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_job_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a91d853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded to Snowflake table mart_top_job_skills successfully.\n"
     ]
    }
   ],
   "source": [
    "#write to a new snowflake table for seniority\n",
    "\n",
    "def load_to_snowflake(df_job_analysis):\n",
    "    # Create a Snowflake connection engine\n",
    "   engine = create_engine(\n",
    "        'snowflake://{user}:{password}@{account}/{database}/{schema}?warehouse={warehouse}'.format(\n",
    "        user=\"NIKKILW2025\",\n",
    "        password=snowflake_password,\n",
    "        account=\"gbszkwp-by30611\",\n",
    "        warehouse=\"SNOWFLAKE_LEARNING_WH\",\n",
    "        database=\"linkedin_db\",\n",
    "        schema=\"linkedin_raw\"\n",
    "    )\n",
    "   )\n",
    "\n",
    "   table_name = \"mart_top_job_skills\"\n",
    "\n",
    "   df_job_analysis.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        if_exists='replace',\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "   print(f\"Data loaded to Snowflake table {table_name} successfully.\")\n",
    "\n",
    "\n",
    "load_to_snowflake(df_job_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495a749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Poetry (linkedin_etl_project)",
   "language": "python",
   "name": "linkedin_etl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
